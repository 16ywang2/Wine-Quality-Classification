{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from prettytable import PrettyTable\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "wine = pd.read_csv(\"wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine[\"y\"] = np.where(wine[\"quality\"] == \"good\", 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.drop(\"quality\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe8f6d45410>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cor plot\n",
    "cor = wine.corr()\n",
    "f, ax = plt.subplots(figsize = (15,8))\n",
    "cmap = sns.diverging_palette(220,20, as_cmap = True)\n",
    "sns.heatmap(cor, cmap = cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "train, test = train_test_split(wine, test_size = 0.1, random_state = 7)\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "train.drop(\"index\", axis = 1, inplace = True)\n",
    "test.drop(\"index\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train = train.drop(\"y\", axis = 1)\n",
    "X_full_test = test.drop(\"y\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.516626\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>   <td>  1439</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  1428</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    10</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 10 Oct 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.2534</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>20:52:14</td>     <th>  Log-Likelihood:    </th>  <td> -743.42</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -995.78</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.331e-102</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fixed acidity</th>        <td>    0.0925</td> <td>    0.066</td> <td>    1.402</td> <td> 0.161</td> <td>   -0.037</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>volatile acidity</th>     <td>   -3.2395</td> <td>    0.506</td> <td>   -6.406</td> <td> 0.000</td> <td>   -4.231</td> <td>   -2.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>citric acid</th>          <td>   -1.1968</td> <td>    0.587</td> <td>   -2.037</td> <td> 0.042</td> <td>   -2.348</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>residual sugar</th>       <td>    0.0584</td> <td>    0.044</td> <td>    1.321</td> <td> 0.186</td> <td>   -0.028</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chlorides</th>            <td>   -4.1482</td> <td>    1.622</td> <td>   -2.557</td> <td> 0.011</td> <td>   -7.328</td> <td>   -0.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>free sulfur dioxide</th>  <td>    0.0261</td> <td>    0.009</td> <td>    3.051</td> <td> 0.002</td> <td>    0.009</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total sulfur dioxide</th> <td>   -0.0175</td> <td>    0.003</td> <td>   -5.765</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>density</th>              <td>   -6.4451</td> <td>    2.491</td> <td>   -2.587</td> <td> 0.010</td> <td>  -11.328</td> <td>   -1.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pH</th>                   <td>   -0.8118</td> <td>    0.633</td> <td>   -1.283</td> <td> 0.199</td> <td>   -2.052</td> <td>    0.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sulphates</th>            <td>    2.6796</td> <td>    0.457</td> <td>    5.864</td> <td> 0.000</td> <td>    1.784</td> <td>    3.575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alcohol</th>              <td>    0.9026</td> <td>    0.078</td> <td>   11.551</td> <td> 0.000</td> <td>    0.749</td> <td>    1.056</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1439\n",
       "Model:                          Logit   Df Residuals:                     1428\n",
       "Method:                           MLE   Df Model:                           10\n",
       "Date:                Sat, 10 Oct 2020   Pseudo R-squ.:                  0.2534\n",
       "Time:                        20:52:14   Log-Likelihood:                -743.42\n",
       "converged:                       True   LL-Null:                       -995.78\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.331e-102\n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "fixed acidity            0.0925      0.066      1.402      0.161      -0.037       0.222\n",
       "volatile acidity        -3.2395      0.506     -6.406      0.000      -4.231      -2.248\n",
       "citric acid             -1.1968      0.587     -2.037      0.042      -2.348      -0.045\n",
       "residual sugar           0.0584      0.044      1.321      0.186      -0.028       0.145\n",
       "chlorides               -4.1482      1.622     -2.557      0.011      -7.328      -0.969\n",
       "free sulfur dioxide      0.0261      0.009      3.051      0.002       0.009       0.043\n",
       "total sulfur dioxide    -0.0175      0.003     -5.765      0.000      -0.023      -0.012\n",
       "density                 -6.4451      2.491     -2.587      0.010     -11.328      -1.562\n",
       "pH                      -0.8118      0.633     -1.283      0.199      -2.052       0.428\n",
       "sulphates                2.6796      0.457      5.864      0.000       1.784       3.575\n",
       "alcohol                  0.9026      0.078     11.551      0.000       0.749       1.056\n",
       "========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = sm.Logit(train[\"y\"], X_full_train).fit()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is 0.749131341209173\n",
      "The accuracy on the test set is 0.73125\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the train and the test sets and calculate accuracy\n",
    "yhat_train = model1.predict(X_full_train)\n",
    "yhat_test = model1.predict(X_full_test)\n",
    "prediction_train = list(map(round,yhat_train))\n",
    "prediction_test = list(map(round,yhat_test))\n",
    "predictions_train = pd.concat([train[\"y\"], pd.Series(prediction_train)], axis =1)\n",
    "predictions_test = pd.concat([test[\"y\"], pd.Series(prediction_test)], axis =1)\n",
    "predictions_train.rename(columns = {0:\"model1\"}, inplace = True)\n",
    "predictions_test.rename(columns = {0:\"model1\"}, inplace = True)\n",
    "print(f'The accuracy on the training set is {accuracy_score(predictions_train[\"y\"], predictions_train[\"model1\"])}')\n",
    "print(f'The accuracy on the test set is {accuracy_score(predictions_test[\"y\"], predictions_test[\"model1\"])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try take out ph as it correlates strongly with other predictors\n",
    "X_noph_train = X_full_train.drop(\"pH\", axis = 1)\n",
    "X_noph_test = X_full_test.drop(\"pH\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.517198\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>   <td>  1439</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  1429</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     9</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 10 Oct 2020</td> <th>  Pseudo R-squ.:     </th>   <td>0.2526</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>20:52:14</td>     <th>  Log-Likelihood:    </th>  <td> -744.25</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -995.78</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.265e-102</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fixed acidity</th>        <td>    0.1420</td> <td>    0.054</td> <td>    2.642</td> <td> 0.008</td> <td>    0.037</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>volatile acidity</th>     <td>   -3.3245</td> <td>    0.502</td> <td>   -6.622</td> <td> 0.000</td> <td>   -4.308</td> <td>   -2.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>citric acid</th>          <td>   -1.2122</td> <td>    0.587</td> <td>   -2.066</td> <td> 0.039</td> <td>   -2.362</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>residual sugar</th>       <td>    0.0588</td> <td>    0.044</td> <td>    1.333</td> <td> 0.183</td> <td>   -0.028</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chlorides</th>            <td>   -3.6230</td> <td>    1.564</td> <td>   -2.316</td> <td> 0.021</td> <td>   -6.689</td> <td>   -0.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>free sulfur dioxide</th>  <td>    0.0246</td> <td>    0.008</td> <td>    2.892</td> <td> 0.004</td> <td>    0.008</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total sulfur dioxide</th> <td>   -0.0167</td> <td>    0.003</td> <td>   -5.619</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>density</th>              <td>   -9.3811</td> <td>    1.012</td> <td>   -9.271</td> <td> 0.000</td> <td>  -11.364</td> <td>   -7.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sulphates</th>            <td>    2.6653</td> <td>    0.451</td> <td>    5.912</td> <td> 0.000</td> <td>    1.782</td> <td>    3.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alcohol</th>              <td>    0.8856</td> <td>    0.077</td> <td>   11.534</td> <td> 0.000</td> <td>    0.735</td> <td>    1.036</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1439\n",
       "Model:                          Logit   Df Residuals:                     1429\n",
       "Method:                           MLE   Df Model:                            9\n",
       "Date:                Sat, 10 Oct 2020   Pseudo R-squ.:                  0.2526\n",
       "Time:                        20:52:14   Log-Likelihood:                -744.25\n",
       "converged:                       True   LL-Null:                       -995.78\n",
       "Covariance Type:            nonrobust   LLR p-value:                1.265e-102\n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "fixed acidity            0.1420      0.054      2.642      0.008       0.037       0.247\n",
       "volatile acidity        -3.3245      0.502     -6.622      0.000      -4.308      -2.340\n",
       "citric acid             -1.2122      0.587     -2.066      0.039      -2.362      -0.062\n",
       "residual sugar           0.0588      0.044      1.333      0.183      -0.028       0.145\n",
       "chlorides               -3.6230      1.564     -2.316      0.021      -6.689      -0.557\n",
       "free sulfur dioxide      0.0246      0.008      2.892      0.004       0.008       0.041\n",
       "total sulfur dioxide    -0.0167      0.003     -5.619      0.000      -0.023      -0.011\n",
       "density                 -9.3811      1.012     -9.271      0.000     -11.364      -7.398\n",
       "sulphates                2.6653      0.451      5.912      0.000       1.782       3.549\n",
       "alcohol                  0.8856      0.077     11.534      0.000       0.735       1.036\n",
       "========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and predict\n",
    "model2 = sm.Logit(train[\"y\"], X_noph_train).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is 0.7498262682418346\n",
      "The accuracy on the test set is 0.73125\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the train and the test sets and calculate accuracy\n",
    "yhat_train = model2.predict(X_noph_train)\n",
    "yhat_test = model2.predict(X_noph_test)\n",
    "prediction_train = list(map(round,yhat_train))\n",
    "prediction_test = list(map(round,yhat_test))\n",
    "predictions_train = pd.concat([predictions_train, pd.Series(prediction_train)], axis =1)\n",
    "predictions_test = pd.concat([predictions_test, pd.Series(prediction_test)], axis =1)\n",
    "predictions_train.rename(columns = {0:\"model2\"}, inplace = True)\n",
    "predictions_test.rename(columns = {0:\"model2\"}, inplace = True)\n",
    "print(f'The accuracy on the training set is {accuracy_score(predictions_train[\"y\"], predictions_train[\"model2\"])}')\n",
    "print(f'The accuracy on the test set is {accuracy_score(predictions_test[\"y\"], predictions_test[\"model2\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy from logistic regression is 73%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=2000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a normal random forests with 2000 trees\n",
    "model3 = RandomForestClassifier(n_estimators = 2000)\n",
    "model3.fit(X_full_train, train[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAHiCAYAAADYhhUBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7xcVX3//9eboAEKBBG0gapRGsQLGiWoCCIi9YbWGwpKy8X6pVQtFYsWq19LbW3xp61U+SpFC1S0SIWvmIpV1AoogpJAQgKKF4hfCwgiGrmIFfj8/pgdHQ/nMslJ1pw5eT0fj3mcPWuvtfba68wJb9beM5OqQpIkSWpps2EPQJIkSZseQ6gkSZKaM4RKkiSpOUOoJEmSmjOESpIkqTlDqCRJkpozhEqSNilJLkzy2mGPQ9rUGUIlbTKSrE7y8yR39D12mmaf+yX57w01xgGPeUaSv215zIkkOSHJx4Y9jrHG/K5vTnJ6kq3XsY8FSSrJ5htrnNKmzBAqaVPzoqrauu9x4zAHM8oBZwTG/qKq2hp4MrAn8PYhj0dSH0OoJAFJnpbka0l+mmRFkv369h2Z5JtJbk9yXZI/7sp/C/hPYKf+ldWxK5VjV0u7Vbq/SHIVcGeSzbt25yb5UZLrkxwz4LjXrtYdmeQHSX6S5Ogkeya5qjufk/vqH5HkkiQfSLImybeSPLtv/05JliS5Lcl3k/yvvn0nJDknyceS/Aw4GvhL4ODu3FdMNl/9c5Hkz5PckuSmJEf27d8yyT8k+X43vq8m2XKq39FkquqG7vf0+HHmb7Mkb++Od0uSjyaZ1+2+uPv50+789hrkeJIGYwiVtMlLsjNwPvC3wPbAccC5SXbsqtwCvBDYFjgSeF+SJ1fVncDzgRvXY2X1VcCBwHbAfcB/ACuAnYFnA29M8tx1OI2nAguBg4GTgLcBBwCPA16Z5Jlj6l4H7AD8FfB/k2zf7TsL+G9gJ+Ag4O/6QyrwYuCcbtz/AvwdcHZ37k/s6ow7X319/DYwrzvXPwL+T5IHdfveC+wBPJ3e7+ItwH0D/I4mlORhwAuAK8fZfUT3eBbwKGBrYG1o37f7uV13fpdOdSxJgzOEStrUnNetpP00yXld2R8An62qz1bVfVX1BWApveBCVZ1fVd+rnouAC4BnTHMc76+qH1TVz+ldKt6xqt5ZVf9TVdcBHwYOWYf+/qaq7q6qC4A7gbOq6pZuFfArwJP66t4CnFRVv6yqs4FrgQO7sLYP8BddX8uBjwB/2Nf20qo6r5unn483kAHm65fAO7vjfxa4A3h0ks2A1wB/VlU3VNW9VfW1qvoFU/yOJnBekp8CXwUuoheYxzoU+Mequq6q7gDeChwyArcaSCPPPzJJm5qXVNUXx5Q9AnhFkhf1lT0A+DJAkufTWzHcld7/vG8FrJzmOH4w5vg7dYFprTn0wuOgbu7b/vk4z/vflHNDVVXf8+/TW/ncCbitqm4fs2/xBOMe1wDz9eOquqfv+V3d+HYAtgC+N063k/6OJjDe73qsneid41rfp/ffxodO0U7SNBlCJakXrM6sqv81dkeSucC5wGHAp6vql90KaroqNbYNvZXIrfqe//Y4dfrb/QC4vqoWrs/g18POSdIXRB8OLAFuBLZPsk1fEH04cENf27Hn+xvPB5ivydwK3A3sQu/WhH4T/o6m6UZ6AXethwP30AvxO2/gY0nq4+V4SYKPAS9K8twkc5Js0b2B5neABwJzgR8B93SrfM/pa3sz8OC+N7MALAdekGT7JL8NvHGK438D+Fn3ZqUtuzE8PsmeG+wMf9NDgGOSPCDJK4DH0LvU/QPga8Dfd3PwBHr3bH58kr5uBhZ0l9Jh6vmaUFXdB5wG/GP3Bqk5Sfbqgu1kv6PpOAs4Nskj0/sIp7X3uN7TncN99O4VlbSBGUIlbfK68PVieu/0/hG9Vbc3A5t1K4LHAP8O/AR4Nb1Vw7Vtv0UvyFzX3We6E3AmvZW81fTuhzx7iuPfC7wIWARcT29F8CP03ryzMXyd3puYbgXeBRxUVT/u9r0KWEBvhfBTwF91919O5JPdzx8nuWKq+RrAcfQu3V8O3Aa8m97vYcLf0Tr0PZ7T6P2+LqY393cDfwpQVXfRm59Lut/t06Z5LEl98pu3BUmSZrMkRwCvrap9hj0WSZs2V0IlSZLUnCFUkiRJzXk5XpIkSc25EipJkqTmDKGSJElqzg+rHzE77LBDLViwYNjDkCRJmtKyZcturaodx9tnCB0xCxYsYOnSpcMehiRJ0pSSfH+ifV6OlyRJUnOGUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnOGUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnOGUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNbf5sAegdbPyhjUsOP78YQ9DkiSNsNUnHjjsIbgSKkmSpPYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhA6hSSrk+ywHu3OSHLQOtRfkGTVuh5HkiRpFBlCJUmS1JwhtE+S85IsS3J1kqPG2X9YkquSrEhyZlf2iCRf6sq/lOThfU32TfK1JNetXRVNz3uSrEqyMsnBjU5PkiRpxth82AOYYV5TVbcl2RK4PMm5a3ckeRzwNmDvqro1yfbdrpOBj1bVvyZ5DfB+4CXdvvnAPsBuwBLgHOBlwCLgicAO3XEunmxQXSA+CmDOtjtumDOVJEkaIldCf9MxSVYAlwEPAxb27dsfOKeqbgWoqtu68r2Af+u2z6QXOtc6r6ruq6prgId2ZfsAZ1XVvVV1M3ARsOdkg6qqU6tqcVUtnrPVvGmcniRJ0szgSmgnyX7AAcBeVXVXkguBLfqrADVAV/11fjGmff9PSZKkTZYrob82D/hJF0B3A542Zv+XgFcmeTBA3+X4rwGHdNuHAl+d4jgXAwcnmZNkR2Bf4Bsb4gQkSZJGhSuhv/Y54OgkVwHX0rsk/ytVdXWSdwEXJbkXuBI4AjgGOC3Jm4EfAUdOcZxP0buEv4LequlbquqHSRZsuFORJEma2VI1yBVmzRRz5y+s+YefNOxhSJKkEbb6xAObHCfJsqpaPN4+L8dLkiSpOUOoJEmSmjOESpIkqTlDqCRJkpozhEqSJKk5Q6gkSZKaM4RKkiSpOUOoJEmSmjOESpIkqTm/tnPE7L7zPJY2+pYDSZKkjcWVUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnN+RNOIWXnDGhYcf/6whyFJmsRqP0pPmpIroZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5jb5EJrkiCQnT7fOOG3emGSr6Y1OkiRpdtrkQ+hG9EbAECpJkjSOWRlCk/xWkvOTrEiyKsnBSVYn2aHbvzjJheO0OyPJKUm+kuTbSV7Yt3unJJ9L8p0k/19fmw8lWZrk6iR/3ZUdA+wEfDnJl7uy5yS5NMkVST6ZZOuu/MQk1yS5Ksl7N96sSJIkzRybD3sAG8nzgBur6kCAJPOAdw/YdgHwTGAXeiHyd7vyRcCTgF8A1yb5QFX9AHhbVd2WZA7wpSRPqKr3J3kT8KyqurULv28HDqiqO5P8BfCm7hL/S4HdqqqSbLdBzl6SJGmGm5UrocBK4IAk707yjKpasw5t/72q7quq7wDXAbt15V+qqjVVdTdwDfCIrvyVSa4ArgQeBzx2nD6f1pVfkmQ5cHjX/mfA3cBHkrwMuGu8ASU5qlttXXrvXetyKpIkSTPTrFwJrapvJ9kDeAHw90kuAO7h16F7i8maT/D8F31l9wKbJ3kkcBywZ1X9JMkZE/Qd4AtV9ar77UieAjwbOAR4A7D/OOdzKnAqwNz5C8eOT5IkaeTMypXQJDsBd1XVx4D3Ak8GVgN7dFVePknzVyTZLMkuwKOAayepuy1wJ7AmyUOB5/ftux3Yptu+DNh77aX9JFsl2bW7L3ReVX2W3huZFq3DaUqSJI2sWbkSCuwOvCfJfcAvgT8BtgT+JclfAl+fpO21wEXAQ4Gjq+ruJONWrKoVSa4ErqZ36f6Svt2nAv+Z5KaqelaSI4Czkszt9r+dXlD9dJIt6K2WHrteZytJkjRiUuXV3bW6y+mfqapzhj2Wicydv7DmH37SsIchSZrE6hMPHPYQpBkhybKqWjzevll5OV6SJEkz22y9HL9equqIYY9BkiRpU+BKqCRJkpozhEqSJKk5Q6gkSZKaM4RKkiSpOUOoJEmSmjOESpIkqTlDqCRJkpozhEqSJKk5P6x+xOy+8zyW+nVwkiRpxLkSKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas6PaBoxK29Yw4Ljzx/2MCRppKz2o+2kGceVUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnOGUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSczM+hCZZkGTVAHVe3fd8cZL3d9tHJDl5I47vnUkOGKd8vySf6bZ/P8nx3fZLkjx2Y41HkiRpFMyW745fALwa+DeAqloKLG1x4Kp6xwB1lgBLuqcvAT4DXLMxxyVJkjSTNV8JTfLuJK/re35Ckj9Pz3uSrEqyMsnB47RdkOQrSa7oHk/vdp0IPCPJ8iTH9q9Cjmm/Y5Jzk1zePfZeh2OQ5C3d2FYkObErOyPJQd3285J8K8lXgZf1tTsiycldX78PvKcb6y5JruirtzDJsvWYVkmSpJEyjJXQTwAnAR/snr8SeB690LYIeCKwA3B5kovHtL0F+L2qujvJQuAsYDFwPHBcVb0QepfCJzj2PwHvq6qvJnk48HngMYMcI8nz6a1iPrWq7kqyfX+jJFsAHwb2B74LnD324FX1tSRLgM9U1TlduzVJFlXVcuBI4IwJxi5JkjRrNA+hVXVlkock2QnYEfhJVf2/JMcCZ1XVvcDNSS4C9gSu6mv+AODkJIuAe4Fd1/HwBwCPTbL2+bZJtqmq2wc4xgHA6VV1V3cet43pezfg+qr6DkCSjwFHDTCmjwBHJnkTcDDwlLEVkhy1tq852+44QJeSJEkz27DuCT0HOAj4bXorowCZuPqvHAvcTG+1dDPg7nU87mbAXlX18/U4RoCaov+p9o/nXOCvgP8CllXVj+/XadWpwKkAc+cvXJ9jSJIkzSjDenf8J4BD6AXRc7qyi4GDk8xJsiOwL/CNMe3mATdV1X3AHwJzuvLbgW0GOO4FwBvWPulWO8ea6BgXAK9JslXXdvsx7b4FPDLJLt3zV00wht8Ya1XdTe+2gA8Bpw9wDpIkSSNvKCG0qq6mF8RuqKqbuuJP0bv0voLequBbquqHY5p+EDg8yWX0LpPf2ZVfBdzTvWHo2EkOfQy9+zuvSnINcPQ4dcY9RlV9jt473JcmWQ4cN+ac7qZ3yfz87o1J359gDJ8A3pzkyr7A+nF6q6gXTDJ2SZKkWSNVXt0dtiTHAfOq6n9PVXfu/IU1//CTGoxKkmaP1SceOOwhSJukJMuqavF4+2bL54SOrCSfAnah9656SZKkTYIhdMiq6qXDHoMkSVJrM/5rOyVJkjT7GEIlSZLUnCFUkiRJzRlCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnN+TmhI2b3neex1G/+kCRJI86VUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnOGUEmSJDXn54SOmJU3rGHB8ecPexiSNlGr/ZxiSRuIK6GSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJam7SEJpkuySvm6qTJAuSvHrAeqvWZYAT9HNCkuO67d2SLE9yZZJdptt31+fqJDt0219bzz4WJ3n/VP1LkiRtiqZaCd0OmDKEAguAKUPoRvIS4NNV9aSq+t4gDZIM/HWlVfX09RlUVS2tqmPWp60kSdJsN1UIPRHYpVtpfE963pNkVZKVSQ7uq/eMrt6x3YrnV5Jc0T0mDXJJ5ie5uGu/KskzuvI7+uoclOSMMe1eALwReG2SL49daU1yXJITuu0Lk/xdkouAPxvTz4OTXNCtpv4zkL59d3Q/xz33JC9N8sVu//wk307y20n2S/KZAfr/gyTf6M79n5PMmeJ3IkmSNPKmCqHHA9+rqkVV9WbgZcAi4InAAcB7kszv6n2lq/c+4Bbg96rqycDBwLiXpfu8Gvh8Va3te/kgg6+qzwKnAO+rqmcN0GS7qnpmVf3DmPK/Ar5aVU8ClgAPH6ftuOdeVZ8Cfgi8Hvgw8FdV9cNB+k/yGHrzs3d37vcChw5wHpIkSSNt4MvSnX2As6rqXuDmblVxT+BnY+o9ADg5ydpgtesU/V4OnJbkAcB5VTVQCF0PZ09Qvi+9kElVnZ/kJ+PUmejclwB/CqwCLquqs9ah/2cDewCXJwHYkl6A/w1JjgKOApiz7Y4DnKYkSdLMtq7vjs/UVQA4FriZ3qrhYuCBk1WuqovpBbUbgDOTHLZ2V1+1LQY47j385jmNbXPnZMOYou/Jzn1n4D7goUkmmtPx+g/wr90K8qKqenRVnXC/hlWnVtXiqlo8Z6t5UwxTkiRp5psqhN4ObNP3/GLg4CRzkuxILzh+Y5x684Cbquo+4A+BSe9zTPII4Jaq+jDwL8CTu103J3lMF+xeOsD53Aw8pLsHcy7wwgHarD2vQ7uxPB940AR17nfu3ZucTqd3S8E3gTetQ/9fAg5K8pBu3/bdXEiSJM1qk16Or6ofJ7mke7PPfwJvAfYCVtBb2XtLVf0wyY+Be5KsAM4APgicm+QVwJeZfAUSYD/gzUl+CdwBrF0JPR74DPADepe7t55ivL9M8k7g68D1wLemOO5afw2cleQK4CLg/41T51OMf+7voHc/7FeSLKd3af38QfqvqmuSvB24oAvav6R3b+n3Bxy3JEnSSErVVFehNZPMnb+w5h9+0rCHIWkTtfrEA4c9BEkjJMmyqlo83j6/MUmSJEnNGUIlSZLUnCFUkiRJzRlCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnCFUkiRJzRlCJUmS1Nyk3x2vmWf3neex1K/NkyRJI86VUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnN+RNOIWXnDGhYcf/6whyFpSFb7EW2SZglXQiVJktScIVSSJEnNGUIlSZLUnCFUkiRJzRlCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnCFUkiRJzRlCx5HkhCTHbcD+Pptku+7xug3VryRJ0qgyhDZQVS+oqp8C2wGGUEmStMkzhHaSvC3JtUm+CDy6K9slyeeSLEvylSS7deVnJHl/kq8luS7JQV35/CQXJ1meZFWSZ3Tlq5PsAJwI7NLtf0+SM5O8uG8MH0/y+81PXpIkqbHNhz2AmSDJHsAhwJPozckVwDLgVODoqvpOkqcCHwT275rNB/YBdgOWAOcArwY+X1XvSjIH2GrMoY4HHl9Vi7rjPhM4Fvh0knnA04HDN9qJSpIkzRCG0J5nAJ+qqrsAkiwBtqAXCj+ZZG29uX1tzquq+4Brkjy0K7scOC3JA7r9yyc7aFVdlOT/JHkI8DLg3Kq6Z2y9JEcBRwHM2XbH9T1HSZKkGcPL8b9WY55vBvy0qhb1PR7Tt/8XfdsBqKqLgX2BG4Azkxw2wHHPBA4FjgROH3dgVadW1eKqWjxnq3kDno4kSdLMZQjtuRh4aZItk2wDvAi4C7g+ySsA0vPEyTpJ8gjglqr6MPAvwJPHVLkd2GZM2RnAGwGq6urpnogkSdIoMIQCVXUFcDawHDgX+Eq361Dgj5KsAK4GXjx+D7+yH7A8yZXAy4F/GnOcHwOXdG9aek9XdjPwTSZYBZUkSZqNUjX2KrRaSrIVsBJ4clWtmar+3PkLa/7hJ238gUmakVafeOCwhyBJA0uyrKoWj7fPldAhSnIA8C3gA4MEUEmSpNnCd8cPUVV9EXj4sMchSZLUmiuhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas7PCR0xu+88j6V+Y4okSRpxroRKkiSpOUOoJEmSmjOESpIkqTlDqCRJkpozhEqSJKk5Q6gkSZKaM4RKkiSpOT8ndMSsvGENC44/f9jDkGaF1X7mriQNjSuhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmNskQmuSMJAeNU74gyap17GunJOdMsO/CJIvXd5ySJEmzld8dPw1JNq+qG4H7BVpJkiRNbJNYCU1yWJKrkqxIcmZXvG+SryW5boJV0S2SnJ5kZZIrkzyrKz8iySeT/AdwQf/qaZItk3yiO9bZwJZ9/T0nyaVJrujab92Vn5jkmq7Nezf6ZEiSJM0As34lNMnjgLcBe1fVrUm2B/4RmA/sA+wGLAHGXlJ/PUBV7Z5kN3qBc9du317AE6rqtiQL+tr8CXBXVT0hyROAK7ox7AC8HTigqu5M8hfAm5KcDLwU2K2qKsl2E5zDUcBRAHO23XEasyFJkjQzbAorofsD51TVrQBVdVtXfl5V3VdV1wAPHafdPsCZXZtvAd8H1obQL/T1029f4GNdm6uAq7rypwGPBS5Jshw4HHgE8DPgbuAjSV4G3DXeCVTVqVW1uKoWz9lq3uBnLkmSNEPN+pVQIECNU/6LMXXGazeROyfZN96xQi+4vup+O5KnAM8GDgHeQC80S5IkzWqbwkrol4BXJnkwQHc5fhAXA4d2bXYFHg5cuw5tHg88oSu/DNg7ye92+7ZKsmt3X+i8qvos8EZg0cBnJUmSNMJm/UpoVV2d5F3ARUnuBa4csOkHgVOSrATuAY6oql8kky2Q8iHg9CRXAcuBb3Rj+FGSI4Czkszt6r4duB34dJIt6K2WHrtuZydJkjSaUjXe1WPNVHPnL6z5h5807GFIs8LqEw8c9hAkaVZLsqyqxv3M9E3hcrwkSZJmGEOoJEmSmjOESpIkqTlDqCRJkpozhEqSJKk5Q6gkSZKaM4RKkiSpOUOoJEmSmjOESpIkqblZ/7Wds83uO89jqd/yIkmSRpwroZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOb8iKYRs/KGNSw4/vxhD0OaUVb7sWWSNHJcCZUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnOGUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNTcSITTJMUm+meTjSX4/yfEbqN87NkAfE45nbf9JdkpyTre9KMkLpntcSZKkUTYq3x3/OuD5VXV993zJMAfTr6qWMMV4qupG4KDu6SJgMfDZjTw0SZKkGWvGr4QmOQV4FLAkybFJjkhycrfv00kO67b/OMnHu+1dknwuybIkX0myW1f+yCSXJrk8yd9McszzurZXJzmqr/x5Sa5IsiLJl7qy/vGM23+SBUlWJXkg8E7g4CTLkxyc5DtJduzqbZbku0l22LCzKEmSNLPM+JXQqjo6yfOAZ1XVrUmO6Nt9FHBJkuuBPwee1pWfChxdVd9J8lTgg8D+wD8BH6qqjyZ5/SSHfU1V3ZZkS+DyJOfSC+wfBvatquuTbD9Ou0n7r6r/SfIOYHFVvQGgC8iHAicBBwArqurWwWZHkiRpNM34ldDJVNXNwDuALwN/3gXHrYGnA59Mshz4Z2B+12Rv4Kxu+8xJuj4myQrgMuBhwEJ6AffitbcEVNVt47QbtP9+pwGHdduvAU4fWyHJUUmWJll6711rBuxWkiRp5prxK6ED2B34MbBT93wz4KdVtWiC+jVZZ0n2o7ciuVdV3ZXkQmALIFO1HaT/+1Wu+kGSm5PsDzyV3qro2Dqn0lvdZe78hevUvyRJ0kw00iuhSZ4CPB94EnBckkdW1c+A65O8oquTJE/smlwCHNJt3y/sdeYBP+kC6G78+hL/pcAzkzyy63e8y/GD9H87sM2Yso8AHwP+varunaCdJEnSrDGyITTJXHr3aL6me/f5nwOnJQm9APhH3SX1q4EXd83+DHh9ksvphc3xfA7YPMlVwN/QuyRPVf2I3j2o/7fr9+xx2g7S/5eBx659Y1JXtgTYmnEuxUuSJM1GqfLq7rAlWQy8r6qeMVXdufMX1vzDT2owKml0rD7xwGEPQZI0jiTLqmrxePtmwz2hI637oPs/YeLL95IkSbPOyF6Ony2q6sSqekRVfXXYY5EkSWrFECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzm9MGjG77zyPpX5FoSRJGnGuhEqSJKk5Q6gkSZKaM4RKkiSpOUOoJEmSmjOESpIkqTlDqCRJkprzI5pGzMob1rDg+POHPQxpaFb7EWWSNCu4EipJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QOiRJLkyyuO/5giSrhjkmSZKkVgyhkiRJas4QupF1K5zfSvKvSa5Kck6SrYY9LkmSpGHafNgD2EQ8GvijqrokyWnA67ryjyf5ebf9QOC+oYxOkiSpMVdC2/hBVV3SbX8M2KfbPrSqFlXVIuAFEzVOclSSpUmW3nvXmo09VkmSpI3OENpGTfF88sZVp1bV4qpaPGereRtwWJIkScNhCG3j4Un26rZfBXx1mIORJEkaNkNoG98EDk9yFbA98KEhj0eSJGmofGNSG/dV1dFjyvbrf1JVq4HHtxqQJEnSMLkSKkmSpOZcCd3IXOGUJEm6P1dCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnCFUkiRJzRlCJUmS1JyfEzpidt95HktPPHDYw5AkSfhWfooAABISSURBVJoWV0IlSZLUnCFUkiRJzRlCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnJ8TOmJW3rCGBcefP+xhSBvVaj8LV5JmPVdCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnCFUkiRJzRlCJUmS1JwhVJIkSc0ZQiVJktTcJhVCkxyd5LBu+4gkO01S951JDtjY4xhTviDJqo1xTEmSpJlkk/razqo6pe/pEcAq4Max9ZLMqap3NBqHJEnSJmfWroQmOSzJVUlWJDmzKzshyXFJDgIWAx9PsjzJlklWJ3lHkq8Cr0hyRlePJHsm+VrX1zeSbDPmWFsn+VKSK5KsTPLiQcbRbe/R7bsUeH2b2ZEkSRquWbkSmuRxwNuAvavq1iTb9++vqnOSvAE4rqqWdm0A7q6qfbrnz+t+PhA4Gzi4qi5Psi3w8zGHvBt4aVX9LMkOwGVJlgCPnWwcndOBP62qi5K8Z8PMgCRJ0sw2W1dC9wfOqapbAarqtgHbnT1O2aOBm6rq8q6vn1XVPWPqBPi7JFcBXwR2Bh461TiSzAO2q6qLuqIzxxtUkqOSLE2y9N671gx4KpIkSTPXbA2hAWo92t25nn0dCuwI7FFVi4CbgS0GaDvQOKvq1KpaXFWL52w1b6rqkiRJM95sDaFfAl6Z5MEAE1wGvx3YZpzysb4F7JRkz66vbZKMvY1hHnBLVf0yybOARwwyjqr6KbAmyT5d0aEDjEeSJGnkzcp7Qqvq6iTvAi5Kci9wJb13w/c7Azglyc+BvSbp63+SHAx8IMmW9O4HPQC4o6/ax4H/SLIUWE4vuA46jiOB05LcBXx+PU5XkiRp5KRqfa5aa1jmzl9Y8w8/adjDkDaq1SceOOwhSJI2gCTLqmrxePtm6+V4SZIkzWCGUEmSJDVnCJUkSVJzhlBJkiQ1ZwiVJElSc4ZQSZIkNWcIlSRJUnOGUEmSJDVnCJUkSVJzhlBJkiQ1Nyu/O342233neSz1Kw0lSdKIcyVUkiRJzRlCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnB/RNGJW3rCGBcefP+xhSBNa7UeISZIG4EqoJEmSmjOESpIkqTlDqCRJkpozhEqSJKk5Q6gkSZKaM4RKkiSpOUOoJEmSmjOESpIkqTlDqCRJkpozhEqSJKm5dQqhSY5J8s0kH99YAxpwHPsl+Uy3PTfJF5MsT3LwBur/jCQHddsfSfLY9ezna1P1L0mStCla1++Ofx3w/Kq6vr8wyeZVdc+GG9Y6eRLwgKpaNGiDdRlvVb12fQdWVU9f37aSJEmz2cAroUlOAR4FLElybJITkpya5ALgo0nmJHlPksuTXJXkj/vavrmv/K/H6XtOtzq4KsnKJMd25RcmWdxt75Bk9Zh2DwE+BizqVkJ3SbI6yQ7d/sVJLuy2f2O8Y/pJkpOTXJPkfOAhffv6x/Cqbnyrkry7K3tEku9049ssyVeSPKfbd8cA/e+R5KIky5J8Psn8QX8nkiRJo2rgldCqOjrJ84BnVdWtSU4A9gD2qaqfJzkKWFNVeyaZC1zSBb6F3eMpQOiF2H2r6uK+7hcBO1fV4wGSbDfgmG5J8lrguKp6Ydd2sia/Gu+Y8pcCjwZ2Bx4KXAOc1l8hyU7Au7s+fgJckOQlVXVeF0hPAb4OXFNVFwzSf5IHAB8AXlxVP+puJ3gX8JpBzl+SJGlUrevl+LGW9AW65wBP6LvXcR698Pmc7nFlV751V94fQq8DHpXkA8D5wNgQt6EsGSeAAuwLnFVV9wI3JvmvcersCVxYVT8C6O6L3Rc4r6o+kuQVwNH0AvWg/T8aeDzwhS48zwFuGtu4C/hHAczZdseBT1aSJGmmmm4IvbNvO8CfVtXn+yskeS7w91X1zxN1UlU/SfJE4LnA64FX0lsNvIdf3zKwxYBjmqzNnUyspuh3wiXWJFsBv9M93Rq4fcD+A1xdVXtNduCqOhU4FWDu/IVTjVOSJGnG25Af0fR54E+6S8wk2TXJb3Xlr0mydVe+c3cv569093BuVlXnAv8beHK3azW9y98Ag76bvL/NywdsczFwSHdv6nzgWePU+TrwzO7ezznAq4CLun3vBj4OvAP48Dr0fy2wY5K9AJI8IMnjBhyzJEnSyJruSmi/jwALgCvSu7b8I+AlVXVBkscAl3aXnO8A/gC4pa/tzsDpSdaG4rd2P98L/HuSPwTGu0Q+nr8G/iXJX9ILjoP4FLA/sBL4Nr8Ol79SVTcleSvwZXormJ+tqk8neSa9S/V7V9W9SV6e5MiqOn2q/qvqf7rbF96fZB6938dJwNUDjluSJGkkpcqru6Nk7vyFNf/wk4Y9DGlCq088cNhDkCTNEEmWVdXi8fb5jUmSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJam5Dfm2nGth953ks9RtpJEnSiHMlVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnCFUkiRJzRlCJUmS1JwhVJIkSc35OaEjZuUNa1hw/PnDHoY2Qav9fFpJ0gbkSqgkSZKaM4RKkiSpOUOoJEmSmjOESpIkqTlDqCRJkpozhEqSJKk5Q6gkSZKaM4RKkiSpOUOoJEmSmjOESpIkqbmRDaFJjk5y2DjlC5Ksmka/FyZZPL3RSZIkaTIz4rvjkwRIVd03aJuqOmUjDmmokmxeVfcMexySJEkby9BWQrsVy28m+SBwBfCwJM9JcmmSK5J8MsnWXd0Tk1yT5Kok7+3KTkhyXLe9R5IVSS4FXt93jCOSnNz3/DNJ9uu2P5RkaZKrk/z1AOMdbwxnJDmor84d3c/Nknyw6/szST67tl6SdyS5PMmqJKd2AXztCuzfJbkI+LNpTa4kSdIMN+zL8Y8GPlpVTwLuBN4OHFBVTwaWAm9Ksj3wUuBxVfUE4G/H6ed04Jiq2msdjv22qloMPAF4ZpInTFRxwDH0exmwANgdeC3QP66Tq2rPqno8sCXwwr5921XVM6vqH8Yc/6guMC+99641A56eJEnSzDXsEPr9qrqs234a8FjgkiTLgcOBRwA/A+4GPpLkZcBd/R0kmUcvvF3UFZ054LFfmeQK4Ergcd2xJzLpGMaxD/DJqrqvqn4IfLlv37OSfD3JSmD/7thrnT1eZ1V1alUtrqrFc7aaN8WhJUmSZr5h3xN6Z992gC9U1avGVkryFODZwCHAG+iFt/52NUH/9/CbQXuLrr9HAscBe1bVT5KcsXbfeKrqngnG8Kv+u8vqD+wb0/0k2QL4ILC4qn6Q5IQxx71zvHaSJEmzzbBXQvtdBuyd5HcBkmyVZNfuvtB5VfVZ4I3Aov5GVfVTYE2SfbqiQ/t2rwYWdfdoPgx4Sle+Lb3AtybJQ4HnTzawScawGtij234x8IBu+6vAy7vjPhTYrytfGzhv7fr81f2kkiRJm5Jhr4T+SlX9KMkRwFlJ5nbFbwduBz7drSIGOHac5kcCpyW5C/h8X/klwPXASmAVvTdAUVUrklwJXA1c19WbzDYTjOHDXfk3gC/x65XMc+mtmq4Cvg18HVhTVT9N8uFuPKuBy6c4riRJ0qyUqomuZGs6kmxdVXckeTDwDWDv7v7QaZk7f2HNP/yk6Q9QWkerTzxw2EOQJI2YJMu6N4Lfz4xZCZ2FPpNkO3r3if7NhgigkiRJs4UhdCOpqv2GPQZJkqSZaia9MUmSJEmbCEOoJEmSmjOESpIkqTlDqCRJkpozhEqSJKk5Q6gkSZKaM4RKkiSpOT8ndMTsvvM8lvrNNZIkacS5EipJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmDKGSJElqzhAqSZKk5gyhkiRJas4QKkmSpOYMoZIkSWrOECpJkqTmUlXDHoPWQZLbgWuHPY4Zbgfg1mEPYgZzfibn/EzO+ZmaczQ552dys21+HlFVO463Y/PWI9G0XVtVi4c9iJksyVLnaGLOz+Scn8k5P1Nzjibn/ExuU5ofL8dLkiSpOUOoJEmSmjOEjp5Thz2AEeAcTc75mZzzMznnZ2rO0eScn8ltMvPjG5MkSZLUnCuhkiRJas4QOmRJnpfk2iTfTXL8OPvnJjm72//1JAv69r21K782yXMH7XOUrO/8JPm9JMuSrOx+7t/X5sKuz+Xd4yHtzmjDmsb8LEjy8745OKWvzR7dvH03yfuTpN0ZbVjTmJ9D++ZmeZL7kizq9s2a1w8MNEf7JrkiyT1JDhqz7/Ak3+keh/eVb0qvoXHnJ8miJJcmuTrJVUkO7tt3RpLr+15Di1qdz4Y2zdfPvX1zsKSv/JHd3+N3ur/PB7Y4l41hGq+fZ435N+juJC/p9s2a1w9V5WNID2AO8D3gUcADgRXAY8fUeR1wSrd9CHB2t/3Yrv5c4JFdP3MG6XNUHtOcnycBO3Xbjwdu6GtzIbB42Oc35PlZAKyaoN9vAHsBAf4TeP6wz7X1/Iypsztw3Wx7/azDHC0AngB8FDior3x74Lru54O67Qdtgq+hieZnV2Bht70TcBOwXff8jP66o/qYzvx0++6YoN9/Bw7ptk8B/mTY5zqM+emrsz1wG7DVbHr9VJUroUP2FOC7VXVdVf0P8AngxWPqvBj41277HODZ3arCi4FPVNUvqup64Ltdf4P0OSrWe36q6sqqurErvxrYIsncJqNuZzqvn3ElmQ9sW1WXVu9fu48CL9nwQ29iQ83Pq4CzNupIh2fKOaqq1VV1FXDfmLbPBb5QVbdV1U+ALwDP29ReQxPNT1V9u6q+023fCNwCjPuB3SNsOq+fcXV/f/vT+3uE3t/nJvf6GeMg4D+r6q6NN9ThMIQO187AD/qe/3dXNm6dqroHWAM8eJK2g/Q5KqYzP/1eDlxZVb/oKzu9u4zxv0f4UuF05+eRSa5MclGSZ/TV/+8p+hwVG+r1czD3D6Gz4fUD0/v3YrJ/gzal19CUkjyF3krY9/qK39Vdpn/fCP8P8nTnZ4skS5NctvZSM72/v592f4/r0+dMsqH+e3wI9/83aDa8fgyhQzbef7zGflzBRHXWtXwUTWd+ejuTxwHvBv64b/+hVbU78Izu8YfTHOewTGd+bgIeXlVPAt4E/FuSbQfsc1RsiNfPU4G7qmpV3/7Z8vqB6f2+/TdokA56K8NnAkdW1drVrrcCuwF70rvU+hfTGeQQTXd+Hl69bwZ6NXBSkl02QJ8zyYZ6/ewOfL6veLa8fgyhQ/bfwMP6nv8OcONEdZJsDsyjd2/IRG0H6XNUTGd+SPI7wKeAw6rqVysQVXVD9/N24N/oXTIZRes9P91tHD8GqKpl9FZodu3q/84UfY6Kab1+OvdbgZhFrx+Y3r8Xk/0btCm9hibU/Y/d+cDbq+qyteVVdVP1/AI4ndF9DU1rftbeMlVV19G71/pJ9L4zfbvu73Gd+5xhNsR/j18JfKqqfrm2YBa9fgyhQ3Y5sLB7J+AD6f0Hb8mYOkuAte86PQj4r+4+qyXAIem9u/eRwEJ6bwYYpM9Rsd7zk2Q7ev/4v7WqLllbOcnmSXboth8AvBBYxWiazvzsmGQOQJJH0Xv9XFdVNwG3J3lad5n5MODTLU5mI5jO3xdJNgNeQe8+Lrqy2fT6gen9e/F54DlJHpTkQcBzgM9vgq+hcXX1PwV8tKo+OWbf/O5n6N3vOKqvoenMz4PWXkbu/qb2Bq7p/v6+TO/vEXp/n5vc66fP/e5Jn0WvH98dP+wH8ALg2/RWot7Wlb0T+P1uewvgk/TeePQN4FF9bd/WtbuWvnefjtfnqD7Wd36AtwN3Asv7Hg8BfgtYBlxF7w1L/wTMGfZ5DmF+Xt6d/wrgCuBFfX0upveP2veAk+m+1GIUH9P8+9oPuGxMf7Pq9TPgHO1Jb0XnTuDHwNV9bV/Tzd136V1u3hRfQ+POD/AHwC/H/Bu0qNv3X8DKbo4+Bmw97PMcwvw8vZuDFd3PP+rr81Hd3+N3u7/PucM+z9bz0+1bANwAbDamz1nz+vEbkyRJktScl+MlSZLUnCFUkiRJzRlCJUmS1JwhVJIkSc0ZQiVJktScIVSSJEnNGUIlSZLUnCFUkiRJzf3/+7i2PYVKlbQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature importance\n",
    "feature_importance = pd.concat([pd.Series(X_full_train.columns), pd.Series(model3.feature_importances_)], axis = 1).sort_values(by = 1)\n",
    "f, ax = plt.subplots(figsize = (10,8))\n",
    "ax.barh(feature_importance[0], feature_importance[1])\n",
    "ax.set_title(\"Feature Importance Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is 1.0\n",
      "The accuracy on the test set is 0.83125\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the train and the test sets and calculate accuracy\n",
    "yhat_train = model3.predict(X_full_train)\n",
    "yhat_test = model3.predict(X_full_test)\n",
    "predictions_train = pd.concat([predictions_train, pd.Series(yhat_train)], axis =1)\n",
    "predictions_test = pd.concat([predictions_test, pd.Series(yhat_test)], axis =1)\n",
    "predictions_train.rename(columns = {0:\"model3\"}, inplace = True)\n",
    "predictions_test.rename(columns = {0:\"model3\"}, inplace = True)\n",
    "print(f'The accuracy on the training set is {accuracy_score(predictions_train[\"y\"], predictions_train[\"model3\"])}')\n",
    "print(f'The accuracy on the test set is {accuracy_score(predictions_test[\"y\"], predictions_test[\"model3\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using GridCV to optimize our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GridSearch to find the best sets of parameters\n",
    "parameters = {\"max_features\": [\"sqrt\", 5, 7, 9], \"min_samples_leaf\": [1, 2, 4, 6]}\n",
    "gridsearch = GridSearchCV(RandomForestClassifier(n_estimators = 500), parameters, scoring = \"accuracy\")\n",
    "grid_result = gridsearch.fit(X_full_train, train[\"y\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.8283512388695315 with {'max_features': 5, 'min_samples_leaf': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best score is {grid_result.best_score_} with {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the best model is the default one, and the test accuracy is around 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=2000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0.5, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = XGBClassifier(n_estimators = 2000, max_depth = 5, reg_alpha = 0.5)\n",
    "model4.fit(X_full_train, train[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is 1.0\n",
      "The accuracy on the test set is 0.8\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the train and the test sets and calculate accuracy\n",
    "yhat_train = model4.predict(X_full_train)\n",
    "yhat_test = model4.predict(X_full_test)\n",
    "predictions_train = pd.concat([predictions_train, pd.Series(yhat_train)], axis =1)\n",
    "predictions_test = pd.concat([predictions_test, pd.Series(yhat_test)], axis =1)\n",
    "predictions_train.rename(columns = {0:\"model4\"}, inplace = True)\n",
    "predictions_test.rename(columns = {0:\"model4\"}, inplace = True)\n",
    "print(f'The accuracy on the training set is {accuracy_score(predictions_train[\"y\"], predictions_train[\"model4\"])}')\n",
    "print(f'The accuracy on the test set is {accuracy_score(predictions_test[\"y\"], predictions_test[\"model4\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the parameters and trying out different values, the best model is with max_depth of 5 and L1 regularization with alpha = 0.5. The accuracy on test set is 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is 0.7435782036391793 with {'C': 0.5, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# try linear and gaussian kernel with different C\n",
    "parameters = {\"kernel\": [\"linear\", \"rbf\"], \"C\": [0.1, 0.5, 1, 2, 4, 6, 8, 10,12]}\n",
    "gridsearch = GridSearchCV(svm.SVC(), parameters, scoring = \"accuracy\")\n",
    "grid_result = gridsearch.fit(X_full_train, train[\"y\"])\n",
    "print(f\"The best score is {grid_result.best_score_} with {grid_result.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is 0.7484364141765114\n",
      "The accuracy on the test set is 0.725\n"
     ]
    }
   ],
   "source": [
    "# fit and predict\n",
    "model5 = svm.SVC(kernel = \"linear\", C = 0.5)\n",
    "model5.fit(X_full_train, train[\"y\"])\n",
    "yhat_train = model5.predict(X_full_train)\n",
    "yhat_test = model5.predict(X_full_test)\n",
    "predictions_train = pd.concat([predictions_train, pd.Series(yhat_train)], axis =1)\n",
    "predictions_test = pd.concat([predictions_test, pd.Series(yhat_test)], axis =1)\n",
    "predictions_train.rename(columns = {0:\"model5\"}, inplace = True)\n",
    "predictions_test.rename(columns = {0:\"model5\"}, inplace = True)\n",
    "print(f'The accuracy on the training set is {accuracy_score(predictions_train[\"y\"], yhat_train)}')\n",
    "print(f'The accuracy on the test set is {accuracy_score(predictions_test[\"y\"], yhat_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel somehow performs better than the gaussian kernel, but it's worse than our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    x_input = Input(input_shape)\n",
    "    X = Dense(16, activation = \"relu\")(x_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(32, activation = \"relu\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(rate = 0.1)(X)\n",
    "    X = Dense(128, activation = \"relu\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(rate = 0.1)(X)\n",
    "    X = Dense(2, activation = \"sigmoid\")(X)\n",
    "    model = Model(inputs = x_input, outputs = X)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X,y to numpy arrays\n",
    "X_train_nn = X_full_train.to_numpy(dtype = \"float32\")\n",
    "X_test_nn = X_full_test.to_numpy(dtype = \"float32\")\n",
    "y_train_nn = train[\"y\"].to_numpy(dtype = \"float32\")\n",
    "y_test_nn = test[\"y\"].to_numpy(dtype = \"float32\")\n",
    "\n",
    "# change y to one-hot vectors\n",
    "y_train_nn_onehot = to_categorical(y_train_nn, 2)\n",
    "y_test_nn_onehot = to_categorical(y_test_nn, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and compile\n",
    "model6 = model(X_train_nn.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "model6.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1439 samples, validate on 160 samples\n",
      "Epoch 1/300\n",
      "1439/1439 [==============================] - 1s 653us/step - loss: 0.7433 - accuracy: 0.5771 - val_loss: 0.7415 - val_accuracy: 0.4313\n",
      "Epoch 2/300\n",
      "1439/1439 [==============================] - 0s 80us/step - loss: 0.6473 - accuracy: 0.6557 - val_loss: 0.6696 - val_accuracy: 0.6281\n",
      "Epoch 3/300\n",
      "1439/1439 [==============================] - 0s 88us/step - loss: 0.6134 - accuracy: 0.6831 - val_loss: 0.6493 - val_accuracy: 0.6656\n",
      "Epoch 4/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.6236 - accuracy: 0.6574 - val_loss: 0.6331 - val_accuracy: 0.6812\n",
      "Epoch 5/300\n",
      "1439/1439 [==============================] - 0s 66us/step - loss: 0.6105 - accuracy: 0.6734 - val_loss: 0.6234 - val_accuracy: 0.6875\n",
      "Epoch 6/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5873 - accuracy: 0.6942 - val_loss: 0.6127 - val_accuracy: 0.6844\n",
      "Epoch 7/300\n",
      "1439/1439 [==============================] - 0s 74us/step - loss: 0.5815 - accuracy: 0.6932 - val_loss: 0.6101 - val_accuracy: 0.6750\n",
      "Epoch 8/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.5749 - accuracy: 0.7060 - val_loss: 0.6006 - val_accuracy: 0.6656\n",
      "Epoch 9/300\n",
      "1439/1439 [==============================] - 0s 72us/step - loss: 0.5773 - accuracy: 0.7015 - val_loss: 0.5961 - val_accuracy: 0.6531\n",
      "Epoch 10/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.5752 - accuracy: 0.7019 - val_loss: 0.5874 - val_accuracy: 0.6656\n",
      "Epoch 11/300\n",
      "1439/1439 [==============================] - 0s 60us/step - loss: 0.5686 - accuracy: 0.7015 - val_loss: 0.5755 - val_accuracy: 0.6687\n",
      "Epoch 12/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.5647 - accuracy: 0.7054 - val_loss: 0.5679 - val_accuracy: 0.6562\n",
      "Epoch 13/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5569 - accuracy: 0.7199 - val_loss: 0.5585 - val_accuracy: 0.6812\n",
      "Epoch 14/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.5538 - accuracy: 0.7186 - val_loss: 0.5506 - val_accuracy: 0.6906\n",
      "Epoch 15/300\n",
      "1439/1439 [==============================] - 0s 43us/step - loss: 0.5382 - accuracy: 0.7248 - val_loss: 0.5448 - val_accuracy: 0.6687\n",
      "Epoch 16/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.5477 - accuracy: 0.7199 - val_loss: 0.5364 - val_accuracy: 0.7000\n",
      "Epoch 17/300\n",
      "1439/1439 [==============================] - 0s 45us/step - loss: 0.5444 - accuracy: 0.7144 - val_loss: 0.5257 - val_accuracy: 0.7000\n",
      "Epoch 18/300\n",
      "1439/1439 [==============================] - 0s 44us/step - loss: 0.5383 - accuracy: 0.7186 - val_loss: 0.5274 - val_accuracy: 0.7094\n",
      "Epoch 19/300\n",
      "1439/1439 [==============================] - 0s 39us/step - loss: 0.5497 - accuracy: 0.7231 - val_loss: 0.5141 - val_accuracy: 0.7219\n",
      "Epoch 20/300\n",
      "1439/1439 [==============================] - 0s 47us/step - loss: 0.5290 - accuracy: 0.7349 - val_loss: 0.5329 - val_accuracy: 0.7125\n",
      "Epoch 21/300\n",
      "1439/1439 [==============================] - 0s 45us/step - loss: 0.5480 - accuracy: 0.7144 - val_loss: 0.5234 - val_accuracy: 0.7219\n",
      "Epoch 22/300\n",
      "1439/1439 [==============================] - 0s 43us/step - loss: 0.5423 - accuracy: 0.7269 - val_loss: 0.5134 - val_accuracy: 0.7250\n",
      "Epoch 23/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.5423 - accuracy: 0.7234 - val_loss: 0.5174 - val_accuracy: 0.7281\n",
      "Epoch 24/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5330 - accuracy: 0.7359 - val_loss: 0.5018 - val_accuracy: 0.7156\n",
      "Epoch 25/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5215 - accuracy: 0.7338 - val_loss: 0.5208 - val_accuracy: 0.7188\n",
      "Epoch 26/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.5376 - accuracy: 0.7269 - val_loss: 0.5191 - val_accuracy: 0.7156\n",
      "Epoch 27/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.5455 - accuracy: 0.7293 - val_loss: 0.5146 - val_accuracy: 0.7250\n",
      "Epoch 28/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.5362 - accuracy: 0.7227 - val_loss: 0.5279 - val_accuracy: 0.7125\n",
      "Epoch 29/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.5265 - accuracy: 0.7182 - val_loss: 0.5177 - val_accuracy: 0.7344\n",
      "Epoch 30/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5288 - accuracy: 0.7328 - val_loss: 0.5025 - val_accuracy: 0.7250\n",
      "Epoch 31/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.5340 - accuracy: 0.7345 - val_loss: 0.5115 - val_accuracy: 0.7375\n",
      "Epoch 32/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.5262 - accuracy: 0.7338 - val_loss: 0.5602 - val_accuracy: 0.6844\n",
      "Epoch 33/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.5316 - accuracy: 0.7349 - val_loss: 0.5170 - val_accuracy: 0.7344\n",
      "Epoch 34/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.5340 - accuracy: 0.7304 - val_loss: 0.5171 - val_accuracy: 0.7375\n",
      "Epoch 35/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5347 - accuracy: 0.7394 - val_loss: 0.5162 - val_accuracy: 0.7406\n",
      "Epoch 36/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.5338 - accuracy: 0.7373 - val_loss: 0.5212 - val_accuracy: 0.7219\n",
      "Epoch 37/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.5088 - accuracy: 0.7550 - val_loss: 0.5232 - val_accuracy: 0.7250\n",
      "Epoch 38/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.5264 - accuracy: 0.7429 - val_loss: 0.5260 - val_accuracy: 0.7250\n",
      "Epoch 39/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5210 - accuracy: 0.7391 - val_loss: 0.5242 - val_accuracy: 0.7219\n",
      "Epoch 40/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.5159 - accuracy: 0.7467 - val_loss: 0.5568 - val_accuracy: 0.6906\n",
      "Epoch 41/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.5247 - accuracy: 0.7352 - val_loss: 0.5088 - val_accuracy: 0.7344\n",
      "Epoch 42/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.5144 - accuracy: 0.7491 - val_loss: 0.5413 - val_accuracy: 0.6875\n",
      "Epoch 43/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.5160 - accuracy: 0.7384 - val_loss: 0.5328 - val_accuracy: 0.7125\n",
      "Epoch 44/300\n",
      "1439/1439 [==============================] - 0s 44us/step - loss: 0.5232 - accuracy: 0.7443 - val_loss: 0.5564 - val_accuracy: 0.7000\n",
      "Epoch 45/300\n",
      "1439/1439 [==============================] - 0s 45us/step - loss: 0.5214 - accuracy: 0.7425 - val_loss: 0.5110 - val_accuracy: 0.7469\n",
      "Epoch 46/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.5183 - accuracy: 0.7516 - val_loss: 0.5032 - val_accuracy: 0.7375\n",
      "Epoch 47/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5080 - accuracy: 0.7495 - val_loss: 0.4980 - val_accuracy: 0.7625\n",
      "Epoch 48/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5102 - accuracy: 0.7491 - val_loss: 0.4911 - val_accuracy: 0.7531\n",
      "Epoch 49/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5270 - accuracy: 0.7464 - val_loss: 0.5086 - val_accuracy: 0.7344\n",
      "Epoch 50/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.5255 - accuracy: 0.7380 - val_loss: 0.5069 - val_accuracy: 0.7469\n",
      "Epoch 51/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5115 - accuracy: 0.7502 - val_loss: 0.5118 - val_accuracy: 0.7375\n",
      "Epoch 52/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5016 - accuracy: 0.7498 - val_loss: 0.5088 - val_accuracy: 0.7375\n",
      "Epoch 53/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.5116 - accuracy: 0.7404 - val_loss: 0.5255 - val_accuracy: 0.7312\n",
      "Epoch 54/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.5117 - accuracy: 0.7516 - val_loss: 0.5103 - val_accuracy: 0.7344\n",
      "Epoch 55/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.5078 - accuracy: 0.7530 - val_loss: 0.5685 - val_accuracy: 0.6938\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5103 - accuracy: 0.7439 - val_loss: 0.6048 - val_accuracy: 0.6594\n",
      "Epoch 57/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.5110 - accuracy: 0.7484 - val_loss: 0.5307 - val_accuracy: 0.7188\n",
      "Epoch 58/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.5136 - accuracy: 0.7397 - val_loss: 0.5645 - val_accuracy: 0.6938\n",
      "Epoch 59/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.5217 - accuracy: 0.7470 - val_loss: 0.5284 - val_accuracy: 0.7219\n",
      "Epoch 60/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.5193 - accuracy: 0.7460 - val_loss: 0.5109 - val_accuracy: 0.7344\n",
      "Epoch 61/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.5136 - accuracy: 0.7460 - val_loss: 0.4954 - val_accuracy: 0.7531\n",
      "Epoch 62/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5124 - accuracy: 0.7443 - val_loss: 0.4877 - val_accuracy: 0.7625\n",
      "Epoch 63/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.5102 - accuracy: 0.7526 - val_loss: 0.4979 - val_accuracy: 0.7437\n",
      "Epoch 64/300\n",
      "1439/1439 [==============================] - 0s 47us/step - loss: 0.5111 - accuracy: 0.7443 - val_loss: 0.4960 - val_accuracy: 0.7312\n",
      "Epoch 65/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4979 - accuracy: 0.7582 - val_loss: 0.5355 - val_accuracy: 0.7063\n",
      "Epoch 66/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.5134 - accuracy: 0.7453 - val_loss: 0.4999 - val_accuracy: 0.7625\n",
      "Epoch 67/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5090 - accuracy: 0.7460 - val_loss: 0.5157 - val_accuracy: 0.7375\n",
      "Epoch 68/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.5117 - accuracy: 0.7474 - val_loss: 0.5205 - val_accuracy: 0.7188\n",
      "Epoch 69/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.5102 - accuracy: 0.7557 - val_loss: 0.5177 - val_accuracy: 0.7469\n",
      "Epoch 70/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.5016 - accuracy: 0.7578 - val_loss: 0.5112 - val_accuracy: 0.7312\n",
      "Epoch 71/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.5113 - accuracy: 0.7467 - val_loss: 0.5386 - val_accuracy: 0.7156\n",
      "Epoch 72/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.5018 - accuracy: 0.7582 - val_loss: 0.4942 - val_accuracy: 0.7469\n",
      "Epoch 73/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.4989 - accuracy: 0.7627 - val_loss: 0.5133 - val_accuracy: 0.7344\n",
      "Epoch 74/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.4981 - accuracy: 0.7613 - val_loss: 0.5241 - val_accuracy: 0.7375\n",
      "Epoch 75/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.5027 - accuracy: 0.7564 - val_loss: 0.5180 - val_accuracy: 0.7375\n",
      "Epoch 76/300\n",
      "1439/1439 [==============================] - 0s 44us/step - loss: 0.4986 - accuracy: 0.7533 - val_loss: 0.5323 - val_accuracy: 0.7188\n",
      "Epoch 77/300\n",
      "1439/1439 [==============================] - 0s 66us/step - loss: 0.4988 - accuracy: 0.7543 - val_loss: 0.4941 - val_accuracy: 0.7437\n",
      "Epoch 78/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4948 - accuracy: 0.7561 - val_loss: 0.5303 - val_accuracy: 0.7250\n",
      "Epoch 79/300\n",
      "1439/1439 [==============================] - 0s 66us/step - loss: 0.5027 - accuracy: 0.7533 - val_loss: 0.5043 - val_accuracy: 0.7250\n",
      "Epoch 80/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.5124 - accuracy: 0.7498 - val_loss: 0.5094 - val_accuracy: 0.7375\n",
      "Epoch 81/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4927 - accuracy: 0.7516 - val_loss: 0.5359 - val_accuracy: 0.7094\n",
      "Epoch 82/300\n",
      "1439/1439 [==============================] - 0s 75us/step - loss: 0.5046 - accuracy: 0.7526 - val_loss: 0.5504 - val_accuracy: 0.6812\n",
      "Epoch 83/300\n",
      "1439/1439 [==============================] - 0s 99us/step - loss: 0.5065 - accuracy: 0.7502 - val_loss: 0.5490 - val_accuracy: 0.7031\n",
      "Epoch 84/300\n",
      "1439/1439 [==============================] - 0s 264us/step - loss: 0.4957 - accuracy: 0.7505 - val_loss: 0.5093 - val_accuracy: 0.7375\n",
      "Epoch 85/300\n",
      "1439/1439 [==============================] - 0s 74us/step - loss: 0.5166 - accuracy: 0.7477 - val_loss: 0.5152 - val_accuracy: 0.7406\n",
      "Epoch 86/300\n",
      "1439/1439 [==============================] - 0s 72us/step - loss: 0.4964 - accuracy: 0.7536 - val_loss: 0.5402 - val_accuracy: 0.7188\n",
      "Epoch 87/300\n",
      "1439/1439 [==============================] - 0s 67us/step - loss: 0.4900 - accuracy: 0.7599 - val_loss: 0.4953 - val_accuracy: 0.7344\n",
      "Epoch 88/300\n",
      "1439/1439 [==============================] - 0s 70us/step - loss: 0.4958 - accuracy: 0.7634 - val_loss: 0.5183 - val_accuracy: 0.7500\n",
      "Epoch 89/300\n",
      "1439/1439 [==============================] - 0s 66us/step - loss: 0.5091 - accuracy: 0.7585 - val_loss: 0.5184 - val_accuracy: 0.7437\n",
      "Epoch 90/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4935 - accuracy: 0.7641 - val_loss: 0.5054 - val_accuracy: 0.7375\n",
      "Epoch 91/300\n",
      "1439/1439 [==============================] - 0s 97us/step - loss: 0.4848 - accuracy: 0.7686 - val_loss: 0.5260 - val_accuracy: 0.7344\n",
      "Epoch 92/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4917 - accuracy: 0.7564 - val_loss: 0.5219 - val_accuracy: 0.7344\n",
      "Epoch 93/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4998 - accuracy: 0.7547 - val_loss: 0.5253 - val_accuracy: 0.7156\n",
      "Epoch 94/300\n",
      "1439/1439 [==============================] - 0s 69us/step - loss: 0.5053 - accuracy: 0.7585 - val_loss: 0.5017 - val_accuracy: 0.7531\n",
      "Epoch 95/300\n",
      "1439/1439 [==============================] - 0s 70us/step - loss: 0.4944 - accuracy: 0.7623 - val_loss: 0.5028 - val_accuracy: 0.7469\n",
      "Epoch 96/300\n",
      "1439/1439 [==============================] - 0s 60us/step - loss: 0.4784 - accuracy: 0.7689 - val_loss: 0.5305 - val_accuracy: 0.7156\n",
      "Epoch 97/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4907 - accuracy: 0.7675 - val_loss: 0.5363 - val_accuracy: 0.7219\n",
      "Epoch 98/300\n",
      "1439/1439 [==============================] - 0s 68us/step - loss: 0.4964 - accuracy: 0.7547 - val_loss: 0.5373 - val_accuracy: 0.7250\n",
      "Epoch 99/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4962 - accuracy: 0.7578 - val_loss: 0.5036 - val_accuracy: 0.7500\n",
      "Epoch 100/300\n",
      "1439/1439 [==============================] - 0s 60us/step - loss: 0.4933 - accuracy: 0.7648 - val_loss: 0.5011 - val_accuracy: 0.7375\n",
      "Epoch 101/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4741 - accuracy: 0.7724 - val_loss: 0.5199 - val_accuracy: 0.7406\n",
      "Epoch 102/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4941 - accuracy: 0.7575 - val_loss: 0.5608 - val_accuracy: 0.6969\n",
      "Epoch 103/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4912 - accuracy: 0.7571 - val_loss: 0.5423 - val_accuracy: 0.7281\n",
      "Epoch 104/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4893 - accuracy: 0.7557 - val_loss: 0.5564 - val_accuracy: 0.7094\n",
      "Epoch 105/300\n",
      "1439/1439 [==============================] - 0s 88us/step - loss: 0.4858 - accuracy: 0.7578 - val_loss: 0.5972 - val_accuracy: 0.6469\n",
      "Epoch 106/300\n",
      "1439/1439 [==============================] - 0s 83us/step - loss: 0.4827 - accuracy: 0.7707 - val_loss: 0.6353 - val_accuracy: 0.6812\n",
      "Epoch 107/300\n",
      "1439/1439 [==============================] - 0s 78us/step - loss: 0.4815 - accuracy: 0.7700 - val_loss: 0.5755 - val_accuracy: 0.6906\n",
      "Epoch 108/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4892 - accuracy: 0.7519 - val_loss: 0.5940 - val_accuracy: 0.6687\n",
      "Epoch 109/300\n",
      "1439/1439 [==============================] - 0s 87us/step - loss: 0.4863 - accuracy: 0.7488 - val_loss: 0.4976 - val_accuracy: 0.7312\n",
      "Epoch 110/300\n",
      "1439/1439 [==============================] - 0s 88us/step - loss: 0.4820 - accuracy: 0.7675 - val_loss: 0.5078 - val_accuracy: 0.7406\n",
      "Epoch 111/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4951 - accuracy: 0.7637 - val_loss: 0.5043 - val_accuracy: 0.7375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4751 - accuracy: 0.7724 - val_loss: 0.5983 - val_accuracy: 0.6969\n",
      "Epoch 113/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4885 - accuracy: 0.7613 - val_loss: 0.5335 - val_accuracy: 0.7281\n",
      "Epoch 114/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4867 - accuracy: 0.7620 - val_loss: 0.5276 - val_accuracy: 0.7281\n",
      "Epoch 115/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4866 - accuracy: 0.7669 - val_loss: 0.5027 - val_accuracy: 0.7437\n",
      "Epoch 116/300\n",
      "1439/1439 [==============================] - 0s 97us/step - loss: 0.4818 - accuracy: 0.7707 - val_loss: 0.5350 - val_accuracy: 0.7094\n",
      "Epoch 117/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4825 - accuracy: 0.7710 - val_loss: 0.5230 - val_accuracy: 0.7031\n",
      "Epoch 118/300\n",
      "1439/1439 [==============================] - 0s 45us/step - loss: 0.4768 - accuracy: 0.7735 - val_loss: 0.4816 - val_accuracy: 0.7625\n",
      "Epoch 119/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4794 - accuracy: 0.7738 - val_loss: 0.5323 - val_accuracy: 0.7281\n",
      "Epoch 120/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4655 - accuracy: 0.7776 - val_loss: 0.5117 - val_accuracy: 0.7312\n",
      "Epoch 121/300\n",
      "1439/1439 [==============================] - 0s 60us/step - loss: 0.4745 - accuracy: 0.7759 - val_loss: 0.4989 - val_accuracy: 0.7437\n",
      "Epoch 122/300\n",
      "1439/1439 [==============================] - 0s 66us/step - loss: 0.4774 - accuracy: 0.7707 - val_loss: 0.5110 - val_accuracy: 0.7312\n",
      "Epoch 123/300\n",
      "1439/1439 [==============================] - 0s 69us/step - loss: 0.4677 - accuracy: 0.7745 - val_loss: 0.5630 - val_accuracy: 0.6875\n",
      "Epoch 124/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4664 - accuracy: 0.7728 - val_loss: 0.5043 - val_accuracy: 0.7188\n",
      "Epoch 125/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4805 - accuracy: 0.7703 - val_loss: 0.5173 - val_accuracy: 0.7250\n",
      "Epoch 126/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4761 - accuracy: 0.7717 - val_loss: 0.4795 - val_accuracy: 0.7656\n",
      "Epoch 127/300\n",
      "1439/1439 [==============================] - 0s 76us/step - loss: 0.4673 - accuracy: 0.7700 - val_loss: 0.4883 - val_accuracy: 0.7500\n",
      "Epoch 128/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4888 - accuracy: 0.7596 - val_loss: 0.4843 - val_accuracy: 0.7375\n",
      "Epoch 129/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4783 - accuracy: 0.7738 - val_loss: 0.5022 - val_accuracy: 0.7594\n",
      "Epoch 130/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4815 - accuracy: 0.7741 - val_loss: 0.4991 - val_accuracy: 0.7312\n",
      "Epoch 131/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4830 - accuracy: 0.7644 - val_loss: 0.5320 - val_accuracy: 0.7188\n",
      "Epoch 132/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4830 - accuracy: 0.7669 - val_loss: 0.5336 - val_accuracy: 0.7281\n",
      "Epoch 133/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4764 - accuracy: 0.7658 - val_loss: 0.4755 - val_accuracy: 0.7625\n",
      "Epoch 134/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4588 - accuracy: 0.7818 - val_loss: 0.4999 - val_accuracy: 0.7375\n",
      "Epoch 135/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4866 - accuracy: 0.7703 - val_loss: 0.5176 - val_accuracy: 0.7344\n",
      "Epoch 136/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4780 - accuracy: 0.7662 - val_loss: 0.5172 - val_accuracy: 0.7312\n",
      "Epoch 137/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4683 - accuracy: 0.7780 - val_loss: 0.5319 - val_accuracy: 0.7125\n",
      "Epoch 138/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4720 - accuracy: 0.7700 - val_loss: 0.5380 - val_accuracy: 0.7188\n",
      "Epoch 139/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4741 - accuracy: 0.7762 - val_loss: 0.5302 - val_accuracy: 0.7188\n",
      "Epoch 140/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4746 - accuracy: 0.7745 - val_loss: 0.6030 - val_accuracy: 0.6625\n",
      "Epoch 141/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4864 - accuracy: 0.7648 - val_loss: 0.4777 - val_accuracy: 0.7656\n",
      "Epoch 142/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4729 - accuracy: 0.7748 - val_loss: 0.5278 - val_accuracy: 0.7281\n",
      "Epoch 143/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4829 - accuracy: 0.7662 - val_loss: 0.4862 - val_accuracy: 0.7563\n",
      "Epoch 144/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4754 - accuracy: 0.7780 - val_loss: 0.5184 - val_accuracy: 0.7344\n",
      "Epoch 145/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4682 - accuracy: 0.7728 - val_loss: 0.5294 - val_accuracy: 0.7312\n",
      "Epoch 146/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4681 - accuracy: 0.7769 - val_loss: 0.4969 - val_accuracy: 0.7437\n",
      "Epoch 147/300\n",
      "1439/1439 [==============================] - 0s 67us/step - loss: 0.4629 - accuracy: 0.7783 - val_loss: 0.4883 - val_accuracy: 0.7437\n",
      "Epoch 148/300\n",
      "1439/1439 [==============================] - 0s 76us/step - loss: 0.4865 - accuracy: 0.7682 - val_loss: 0.5082 - val_accuracy: 0.7375\n",
      "Epoch 149/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4775 - accuracy: 0.7662 - val_loss: 0.5138 - val_accuracy: 0.7188\n",
      "Epoch 150/300\n",
      "1439/1439 [==============================] - 0s 63us/step - loss: 0.4672 - accuracy: 0.7644 - val_loss: 0.5069 - val_accuracy: 0.7188\n",
      "Epoch 151/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4737 - accuracy: 0.7766 - val_loss: 0.5571 - val_accuracy: 0.6969\n",
      "Epoch 152/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4819 - accuracy: 0.7696 - val_loss: 0.5076 - val_accuracy: 0.7063\n",
      "Epoch 153/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4751 - accuracy: 0.7731 - val_loss: 0.4938 - val_accuracy: 0.7250\n",
      "Epoch 154/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4634 - accuracy: 0.7867 - val_loss: 0.4956 - val_accuracy: 0.7281\n",
      "Epoch 155/300\n",
      "1439/1439 [==============================] - 0s 60us/step - loss: 0.4740 - accuracy: 0.7644 - val_loss: 0.5028 - val_accuracy: 0.7375\n",
      "Epoch 156/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4561 - accuracy: 0.7828 - val_loss: 0.4694 - val_accuracy: 0.7719\n",
      "Epoch 157/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4489 - accuracy: 0.7808 - val_loss: 0.4983 - val_accuracy: 0.7312\n",
      "Epoch 158/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4747 - accuracy: 0.7686 - val_loss: 0.4948 - val_accuracy: 0.7281\n",
      "Epoch 159/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4536 - accuracy: 0.7839 - val_loss: 0.4928 - val_accuracy: 0.7563\n",
      "Epoch 160/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.4671 - accuracy: 0.7780 - val_loss: 0.4712 - val_accuracy: 0.7719\n",
      "Epoch 161/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4623 - accuracy: 0.7735 - val_loss: 0.4871 - val_accuracy: 0.7500\n",
      "Epoch 162/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4778 - accuracy: 0.7714 - val_loss: 0.5016 - val_accuracy: 0.7594\n",
      "Epoch 163/300\n",
      "1439/1439 [==============================] - 0s 63us/step - loss: 0.4771 - accuracy: 0.7665 - val_loss: 0.4683 - val_accuracy: 0.7844\n",
      "Epoch 164/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4594 - accuracy: 0.7835 - val_loss: 0.4847 - val_accuracy: 0.7688\n",
      "Epoch 165/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4597 - accuracy: 0.7773 - val_loss: 0.4801 - val_accuracy: 0.7937\n",
      "Epoch 166/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4680 - accuracy: 0.7728 - val_loss: 0.4768 - val_accuracy: 0.7625\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439/1439 [==============================] - 0s 60us/step - loss: 0.4599 - accuracy: 0.7783 - val_loss: 0.5077 - val_accuracy: 0.7250\n",
      "Epoch 168/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4701 - accuracy: 0.7821 - val_loss: 0.5487 - val_accuracy: 0.7250\n",
      "Epoch 169/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4656 - accuracy: 0.7787 - val_loss: 0.4640 - val_accuracy: 0.7906\n",
      "Epoch 170/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4623 - accuracy: 0.7755 - val_loss: 0.4739 - val_accuracy: 0.7563\n",
      "Epoch 171/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4531 - accuracy: 0.7766 - val_loss: 0.4602 - val_accuracy: 0.7625\n",
      "Epoch 172/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.4588 - accuracy: 0.7787 - val_loss: 0.4947 - val_accuracy: 0.7594\n",
      "Epoch 173/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.4641 - accuracy: 0.7839 - val_loss: 0.4931 - val_accuracy: 0.7625\n",
      "Epoch 174/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4660 - accuracy: 0.7794 - val_loss: 0.5093 - val_accuracy: 0.7375\n",
      "Epoch 175/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4518 - accuracy: 0.7929 - val_loss: 0.4899 - val_accuracy: 0.7781\n",
      "Epoch 176/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4634 - accuracy: 0.7797 - val_loss: 0.5144 - val_accuracy: 0.7063\n",
      "Epoch 177/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4516 - accuracy: 0.7870 - val_loss: 0.4866 - val_accuracy: 0.7781\n",
      "Epoch 178/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4497 - accuracy: 0.7874 - val_loss: 0.4849 - val_accuracy: 0.7500\n",
      "Epoch 179/300\n",
      "1439/1439 [==============================] - 0s 73us/step - loss: 0.4646 - accuracy: 0.7717 - val_loss: 0.4712 - val_accuracy: 0.7844\n",
      "Epoch 180/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4508 - accuracy: 0.7856 - val_loss: 0.4717 - val_accuracy: 0.7688\n",
      "Epoch 181/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4565 - accuracy: 0.7814 - val_loss: 0.4686 - val_accuracy: 0.7656\n",
      "Epoch 182/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4485 - accuracy: 0.7929 - val_loss: 0.5374 - val_accuracy: 0.7000\n",
      "Epoch 183/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4709 - accuracy: 0.7769 - val_loss: 0.5521 - val_accuracy: 0.7094\n",
      "Epoch 184/300\n",
      "1439/1439 [==============================] - 0s 87us/step - loss: 0.4468 - accuracy: 0.7870 - val_loss: 0.5252 - val_accuracy: 0.7250\n",
      "Epoch 185/300\n",
      "1439/1439 [==============================] - 0s 91us/step - loss: 0.4518 - accuracy: 0.7814 - val_loss: 0.5225 - val_accuracy: 0.7469\n",
      "Epoch 186/300\n",
      "1439/1439 [==============================] - 0s 63us/step - loss: 0.4478 - accuracy: 0.7874 - val_loss: 0.5583 - val_accuracy: 0.7250\n",
      "Epoch 187/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4416 - accuracy: 0.7884 - val_loss: 0.4636 - val_accuracy: 0.7781\n",
      "Epoch 188/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4519 - accuracy: 0.7946 - val_loss: 0.5127 - val_accuracy: 0.7437\n",
      "Epoch 189/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4618 - accuracy: 0.7801 - val_loss: 0.5089 - val_accuracy: 0.7656\n",
      "Epoch 190/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4598 - accuracy: 0.7808 - val_loss: 0.4924 - val_accuracy: 0.7594\n",
      "Epoch 191/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4660 - accuracy: 0.7735 - val_loss: 0.4997 - val_accuracy: 0.7344\n",
      "Epoch 192/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4595 - accuracy: 0.7714 - val_loss: 0.5170 - val_accuracy: 0.7469\n",
      "Epoch 193/300\n",
      "1439/1439 [==============================] - 0s 63us/step - loss: 0.4480 - accuracy: 0.7821 - val_loss: 0.4812 - val_accuracy: 0.7875\n",
      "Epoch 194/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4650 - accuracy: 0.7766 - val_loss: 0.4681 - val_accuracy: 0.7688\n",
      "Epoch 195/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4543 - accuracy: 0.7870 - val_loss: 0.4650 - val_accuracy: 0.7656\n",
      "Epoch 196/300\n",
      "1439/1439 [==============================] - 0s 67us/step - loss: 0.4526 - accuracy: 0.7880 - val_loss: 0.4721 - val_accuracy: 0.7563\n",
      "Epoch 197/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4504 - accuracy: 0.7870 - val_loss: 0.4672 - val_accuracy: 0.7812\n",
      "Epoch 198/300\n",
      "1439/1439 [==============================] - 0s 47us/step - loss: 0.4471 - accuracy: 0.7787 - val_loss: 0.4647 - val_accuracy: 0.7719\n",
      "Epoch 199/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4540 - accuracy: 0.7894 - val_loss: 0.4728 - val_accuracy: 0.7750\n",
      "Epoch 200/300\n",
      "1439/1439 [==============================] - 0s 65us/step - loss: 0.4549 - accuracy: 0.7790 - val_loss: 0.4896 - val_accuracy: 0.7437\n",
      "Epoch 201/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4357 - accuracy: 0.7898 - val_loss: 0.4983 - val_accuracy: 0.7563\n",
      "Epoch 202/300\n",
      "1439/1439 [==============================] - 0s 92us/step - loss: 0.4633 - accuracy: 0.7814 - val_loss: 0.4826 - val_accuracy: 0.7563\n",
      "Epoch 203/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4556 - accuracy: 0.7811 - val_loss: 0.4970 - val_accuracy: 0.7219\n",
      "Epoch 204/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4490 - accuracy: 0.7912 - val_loss: 0.5710 - val_accuracy: 0.6875\n",
      "Epoch 205/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4420 - accuracy: 0.7818 - val_loss: 0.5648 - val_accuracy: 0.6906\n",
      "Epoch 206/300\n",
      "1439/1439 [==============================] - 0s 60us/step - loss: 0.4431 - accuracy: 0.7898 - val_loss: 0.4922 - val_accuracy: 0.7500\n",
      "Epoch 207/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4514 - accuracy: 0.7856 - val_loss: 0.5866 - val_accuracy: 0.7031\n",
      "Epoch 208/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4558 - accuracy: 0.7811 - val_loss: 0.4946 - val_accuracy: 0.7688\n",
      "Epoch 209/300\n",
      "1439/1439 [==============================] - 0s 70us/step - loss: 0.4369 - accuracy: 0.7887 - val_loss: 0.4932 - val_accuracy: 0.7625\n",
      "Epoch 210/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4458 - accuracy: 0.7894 - val_loss: 0.4998 - val_accuracy: 0.7563\n",
      "Epoch 211/300\n",
      "1439/1439 [==============================] - 0s 63us/step - loss: 0.4443 - accuracy: 0.7898 - val_loss: 0.4804 - val_accuracy: 0.7469\n",
      "Epoch 212/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4696 - accuracy: 0.7707 - val_loss: 0.5496 - val_accuracy: 0.7219\n",
      "Epoch 213/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4589 - accuracy: 0.7752 - val_loss: 0.4712 - val_accuracy: 0.7625\n",
      "Epoch 214/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4474 - accuracy: 0.7912 - val_loss: 0.4729 - val_accuracy: 0.7688\n",
      "Epoch 215/300\n",
      "1439/1439 [==============================] - 0s 65us/step - loss: 0.4430 - accuracy: 0.7814 - val_loss: 0.5122 - val_accuracy: 0.7281\n",
      "Epoch 216/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4323 - accuracy: 0.7971 - val_loss: 0.4785 - val_accuracy: 0.7688\n",
      "Epoch 217/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4605 - accuracy: 0.7735 - val_loss: 0.5091 - val_accuracy: 0.7375\n",
      "Epoch 218/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4523 - accuracy: 0.7853 - val_loss: 0.4973 - val_accuracy: 0.7844\n",
      "Epoch 219/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4275 - accuracy: 0.7936 - val_loss: 0.5042 - val_accuracy: 0.7625\n",
      "Epoch 220/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4310 - accuracy: 0.7929 - val_loss: 0.4695 - val_accuracy: 0.7875\n",
      "Epoch 221/300\n",
      "1439/1439 [==============================] - 0s 61us/step - loss: 0.4456 - accuracy: 0.7849 - val_loss: 0.4860 - val_accuracy: 0.7625\n",
      "Epoch 222/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.4563 - accuracy: 0.7894 - val_loss: 0.4909 - val_accuracy: 0.7563\n",
      "Epoch 223/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4506 - accuracy: 0.7818 - val_loss: 0.4720 - val_accuracy: 0.7750\n",
      "Epoch 224/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4343 - accuracy: 0.8026 - val_loss: 0.5041 - val_accuracy: 0.7500\n",
      "Epoch 225/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4414 - accuracy: 0.7814 - val_loss: 0.4933 - val_accuracy: 0.7469\n",
      "Epoch 226/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4422 - accuracy: 0.7835 - val_loss: 0.4948 - val_accuracy: 0.7469\n",
      "Epoch 227/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.4528 - accuracy: 0.7887 - val_loss: 0.4797 - val_accuracy: 0.7688\n",
      "Epoch 228/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.4467 - accuracy: 0.7915 - val_loss: 0.4772 - val_accuracy: 0.7563\n",
      "Epoch 229/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.4421 - accuracy: 0.7891 - val_loss: 0.4722 - val_accuracy: 0.7750\n",
      "Epoch 230/300\n",
      "1439/1439 [==============================] - 0s 50us/step - loss: 0.4529 - accuracy: 0.7818 - val_loss: 0.4855 - val_accuracy: 0.7594\n",
      "Epoch 231/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4377 - accuracy: 0.7787 - val_loss: 0.5855 - val_accuracy: 0.7094\n",
      "Epoch 232/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4437 - accuracy: 0.7974 - val_loss: 0.5238 - val_accuracy: 0.7156\n",
      "Epoch 233/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4399 - accuracy: 0.7943 - val_loss: 0.5076 - val_accuracy: 0.7312\n",
      "Epoch 234/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4486 - accuracy: 0.7818 - val_loss: 0.5060 - val_accuracy: 0.7688\n",
      "Epoch 235/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4327 - accuracy: 0.7856 - val_loss: 0.5148 - val_accuracy: 0.7500\n",
      "Epoch 236/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4433 - accuracy: 0.7863 - val_loss: 0.4961 - val_accuracy: 0.7688\n",
      "Epoch 237/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.4485 - accuracy: 0.7839 - val_loss: 0.5395 - val_accuracy: 0.7250\n",
      "Epoch 238/300\n",
      "1439/1439 [==============================] - 0s 66us/step - loss: 0.4273 - accuracy: 0.7887 - val_loss: 0.4779 - val_accuracy: 0.7563\n",
      "Epoch 239/300\n",
      "1439/1439 [==============================] - 0s 70us/step - loss: 0.4399 - accuracy: 0.7915 - val_loss: 0.4601 - val_accuracy: 0.7625\n",
      "Epoch 240/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4512 - accuracy: 0.7891 - val_loss: 0.5403 - val_accuracy: 0.6969\n",
      "Epoch 241/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4297 - accuracy: 0.7901 - val_loss: 0.4916 - val_accuracy: 0.7781\n",
      "Epoch 242/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4318 - accuracy: 0.7943 - val_loss: 0.5090 - val_accuracy: 0.7531\n",
      "Epoch 243/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4393 - accuracy: 0.7919 - val_loss: 0.4978 - val_accuracy: 0.7563\n",
      "Epoch 244/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.4417 - accuracy: 0.7825 - val_loss: 0.4706 - val_accuracy: 0.7750\n",
      "Epoch 245/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4368 - accuracy: 0.7974 - val_loss: 0.5508 - val_accuracy: 0.7469\n",
      "Epoch 246/300\n",
      "1439/1439 [==============================] - 0s 81us/step - loss: 0.4557 - accuracy: 0.7828 - val_loss: 0.4851 - val_accuracy: 0.7875\n",
      "Epoch 247/300\n",
      "1439/1439 [==============================] - 0s 91us/step - loss: 0.4437 - accuracy: 0.7842 - val_loss: 0.5089 - val_accuracy: 0.7688\n",
      "Epoch 248/300\n",
      "1439/1439 [==============================] - 0s 84us/step - loss: 0.4361 - accuracy: 0.7933 - val_loss: 0.4719 - val_accuracy: 0.7563\n",
      "Epoch 249/300\n",
      "1439/1439 [==============================] - 0s 142us/step - loss: 0.4322 - accuracy: 0.7985 - val_loss: 0.5385 - val_accuracy: 0.7437\n",
      "Epoch 250/300\n",
      "1439/1439 [==============================] - 0s 95us/step - loss: 0.4355 - accuracy: 0.7919 - val_loss: 0.5084 - val_accuracy: 0.7688\n",
      "Epoch 251/300\n",
      "1439/1439 [==============================] - 0s 71us/step - loss: 0.4364 - accuracy: 0.7849 - val_loss: 0.4789 - val_accuracy: 0.7688\n",
      "Epoch 252/300\n",
      "1439/1439 [==============================] - 0s 47us/step - loss: 0.4279 - accuracy: 0.7971 - val_loss: 0.5138 - val_accuracy: 0.7594\n",
      "Epoch 253/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4330 - accuracy: 0.7946 - val_loss: 0.5213 - val_accuracy: 0.7344\n",
      "Epoch 254/300\n",
      "1439/1439 [==============================] - 0s 69us/step - loss: 0.4430 - accuracy: 0.7981 - val_loss: 0.4798 - val_accuracy: 0.7531\n",
      "Epoch 255/300\n",
      "1439/1439 [==============================] - 0s 59us/step - loss: 0.4229 - accuracy: 0.8023 - val_loss: 0.6222 - val_accuracy: 0.7219\n",
      "Epoch 256/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.4174 - accuracy: 0.7967 - val_loss: 0.4913 - val_accuracy: 0.7656\n",
      "Epoch 257/300\n",
      "1439/1439 [==============================] - 0s 52us/step - loss: 0.4368 - accuracy: 0.7919 - val_loss: 0.5355 - val_accuracy: 0.7188\n",
      "Epoch 258/300\n",
      "1439/1439 [==============================] - 0s 78us/step - loss: 0.4364 - accuracy: 0.7880 - val_loss: 0.4541 - val_accuracy: 0.7875\n",
      "Epoch 259/300\n",
      "1439/1439 [==============================] - 0s 144us/step - loss: 0.4385 - accuracy: 0.7926 - val_loss: 0.4951 - val_accuracy: 0.7563\n",
      "Epoch 260/300\n",
      "1439/1439 [==============================] - 0s 107us/step - loss: 0.4347 - accuracy: 0.7929 - val_loss: 0.4686 - val_accuracy: 0.7625\n",
      "Epoch 261/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4293 - accuracy: 0.8006 - val_loss: 0.4734 - val_accuracy: 0.7719\n",
      "Epoch 262/300\n",
      "1439/1439 [==============================] - 0s 68us/step - loss: 0.4340 - accuracy: 0.7929 - val_loss: 0.4871 - val_accuracy: 0.7531\n",
      "Epoch 263/300\n",
      "1439/1439 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.79 - 0s 108us/step - loss: 0.4432 - accuracy: 0.7936 - val_loss: 0.5067 - val_accuracy: 0.7437\n",
      "Epoch 264/300\n",
      "1439/1439 [==============================] - 0s 97us/step - loss: 0.4401 - accuracy: 0.7946 - val_loss: 0.5123 - val_accuracy: 0.7469\n",
      "Epoch 265/300\n",
      "1439/1439 [==============================] - 0s 102us/step - loss: 0.4168 - accuracy: 0.7950 - val_loss: 0.4971 - val_accuracy: 0.7312\n",
      "Epoch 266/300\n",
      "1439/1439 [==============================] - 0s 93us/step - loss: 0.4347 - accuracy: 0.7880 - val_loss: 0.5262 - val_accuracy: 0.7188\n",
      "Epoch 267/300\n",
      "1439/1439 [==============================] - 0s 95us/step - loss: 0.4269 - accuracy: 0.8013 - val_loss: 0.4688 - val_accuracy: 0.8000\n",
      "Epoch 268/300\n",
      "1439/1439 [==============================] - 0s 67us/step - loss: 0.4198 - accuracy: 0.7960 - val_loss: 0.4672 - val_accuracy: 0.7688\n",
      "Epoch 269/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4364 - accuracy: 0.7964 - val_loss: 0.4781 - val_accuracy: 0.7531\n",
      "Epoch 270/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4231 - accuracy: 0.7974 - val_loss: 0.4781 - val_accuracy: 0.7625\n",
      "Epoch 271/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4269 - accuracy: 0.7960 - val_loss: 0.4572 - val_accuracy: 0.7750\n",
      "Epoch 272/300\n",
      "1439/1439 [==============================] - 0s 67us/step - loss: 0.4326 - accuracy: 0.7957 - val_loss: 0.4973 - val_accuracy: 0.7625\n",
      "Epoch 273/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.4310 - accuracy: 0.7915 - val_loss: 0.4948 - val_accuracy: 0.7750\n",
      "Epoch 274/300\n",
      "1439/1439 [==============================] - 0s 69us/step - loss: 0.4454 - accuracy: 0.7741 - val_loss: 0.5188 - val_accuracy: 0.7125\n",
      "Epoch 275/300\n",
      "1439/1439 [==============================] - 0s 118us/step - loss: 0.4167 - accuracy: 0.8016 - val_loss: 0.4946 - val_accuracy: 0.7875\n",
      "Epoch 276/300\n",
      "1439/1439 [==============================] - 0s 88us/step - loss: 0.4362 - accuracy: 0.7874 - val_loss: 0.5082 - val_accuracy: 0.7781\n",
      "Epoch 277/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439/1439 [==============================] - 0s 63us/step - loss: 0.4196 - accuracy: 0.8037 - val_loss: 0.4706 - val_accuracy: 0.7750\n",
      "Epoch 278/300\n",
      "1439/1439 [==============================] - 0s 53us/step - loss: 0.4206 - accuracy: 0.7978 - val_loss: 0.4782 - val_accuracy: 0.7563\n",
      "Epoch 279/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4340 - accuracy: 0.7818 - val_loss: 0.5051 - val_accuracy: 0.7531\n",
      "Epoch 280/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4392 - accuracy: 0.7936 - val_loss: 0.4895 - val_accuracy: 0.7812\n",
      "Epoch 281/300\n",
      "1439/1439 [==============================] - 0s 65us/step - loss: 0.4149 - accuracy: 0.8002 - val_loss: 0.5245 - val_accuracy: 0.7437\n",
      "Epoch 282/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4356 - accuracy: 0.7940 - val_loss: 0.5259 - val_accuracy: 0.7437\n",
      "Epoch 283/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4140 - accuracy: 0.8127 - val_loss: 0.5249 - val_accuracy: 0.7406\n",
      "Epoch 284/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4241 - accuracy: 0.8033 - val_loss: 0.4676 - val_accuracy: 0.7750\n",
      "Epoch 285/300\n",
      "1439/1439 [==============================] - 0s 54us/step - loss: 0.4205 - accuracy: 0.7971 - val_loss: 0.4936 - val_accuracy: 0.7688\n",
      "Epoch 286/300\n",
      "1439/1439 [==============================] - 0s 55us/step - loss: 0.4308 - accuracy: 0.7929 - val_loss: 0.5209 - val_accuracy: 0.7375\n",
      "Epoch 287/300\n",
      "1439/1439 [==============================] - 0s 57us/step - loss: 0.4325 - accuracy: 0.8002 - val_loss: 0.5007 - val_accuracy: 0.7875\n",
      "Epoch 288/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4148 - accuracy: 0.8103 - val_loss: 0.4843 - val_accuracy: 0.7437\n",
      "Epoch 289/300\n",
      "1439/1439 [==============================] - 0s 58us/step - loss: 0.4138 - accuracy: 0.7971 - val_loss: 0.4827 - val_accuracy: 0.7812\n",
      "Epoch 290/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4274 - accuracy: 0.7950 - val_loss: 0.5046 - val_accuracy: 0.7719\n",
      "Epoch 291/300\n",
      "1439/1439 [==============================] - 0s 64us/step - loss: 0.4235 - accuracy: 0.7926 - val_loss: 0.4782 - val_accuracy: 0.7750\n",
      "Epoch 292/300\n",
      "1439/1439 [==============================] - 0s 62us/step - loss: 0.4051 - accuracy: 0.8082 - val_loss: 0.5052 - val_accuracy: 0.7625\n",
      "Epoch 293/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4291 - accuracy: 0.8068 - val_loss: 0.5071 - val_accuracy: 0.7531\n",
      "Epoch 294/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4234 - accuracy: 0.8040 - val_loss: 0.4770 - val_accuracy: 0.7563\n",
      "Epoch 295/300\n",
      "1439/1439 [==============================] - 0s 51us/step - loss: 0.4104 - accuracy: 0.8075 - val_loss: 0.5038 - val_accuracy: 0.7500\n",
      "Epoch 296/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4146 - accuracy: 0.8068 - val_loss: 0.5491 - val_accuracy: 0.7188\n",
      "Epoch 297/300\n",
      "1439/1439 [==============================] - 0s 48us/step - loss: 0.4209 - accuracy: 0.7974 - val_loss: 0.5427 - val_accuracy: 0.7406\n",
      "Epoch 298/300\n",
      "1439/1439 [==============================] - 0s 49us/step - loss: 0.4332 - accuracy: 0.7926 - val_loss: 0.4829 - val_accuracy: 0.7750\n",
      "Epoch 299/300\n",
      "1439/1439 [==============================] - 0s 56us/step - loss: 0.4111 - accuracy: 0.8065 - val_loss: 0.4807 - val_accuracy: 0.7688\n",
      "Epoch 300/300\n",
      "1439/1439 [==============================] - 0s 73us/step - loss: 0.4250 - accuracy: 0.7985 - val_loss: 0.5015 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe8d4e18bd0>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.fit(X_train_nn, y_train_nn_onehot, batch_size = 64, epochs = 300, validation_data = (X_test_nn, y_test_nn_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying multiple architectures, it seems like NN can't get above 80%. The limited amount of data is likely the reason. The best performance is around 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is 0.8297428769979153\n",
      "The accuracy on the test set is 0.75\n"
     ]
    }
   ],
   "source": [
    "# append predictions\n",
    "yhat_train = np.argmax(model6.predict(X_train_nn), axis = 1)\n",
    "yhat_test = np.argmax(model6.predict(X_test_nn), axis = 1)\n",
    "predictions_train = pd.concat([predictions_train, pd.Series(yhat_train)], axis = 1)\n",
    "predictions_test = pd.concat([predictions_test, pd.Series(yhat_test)], axis = 1)\n",
    "predictions_train.rename(columns = {0:\"model6\"}, inplace = True)\n",
    "predictions_test.rename(columns = {0:\"model6\"}, inplace = True)\n",
    "print(f'The accuracy on the training set is {accuracy_score(predictions_train[\"y\"], yhat_train)}')\n",
    "print(f'The accuracy on the test set is {accuracy_score(predictions_test[\"y\"], yhat_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>model5</th>\n",
       "      <th>model6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     y  model1  model2  model3  model4  model5  model6\n",
       "0    1       0       0       0       0       0       1\n",
       "1    1       1       1       1       1       0       1\n",
       "2    1       1       1       1       1       1       1\n",
       "3    0       0       0       0       0       0       0\n",
       "4    1       1       1       1       1       1       1\n",
       "..  ..     ...     ...     ...     ...     ...     ...\n",
       "155  0       0       0       0       1       0       0\n",
       "156  0       0       0       0       0       0       0\n",
       "157  1       0       0       0       0       0       0\n",
       "158  0       1       1       1       0       0       0\n",
       "159  1       1       1       1       1       1       1\n",
       "\n",
       "[160 rows x 7 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export test result \n",
    "predictions_test.drop(\"y\", axis = 1, inplace = True)\n",
    "result = pd.concat([test, predictions_test], axis = 1)\n",
    "result.rename(columns = {\"model1\": \"Logistic Regression\", \"model3\": \"Random Forest\", \n",
    "                              \"model4\": \"Gradient Boosted Trees\", \"model5\": \"SVM\", \"model6\": \"Neural Net\"}, inplace = True)\n",
    "result.drop(\"model2\", axis = 1, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"test_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a nice table showing the accuracy of different models\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Method\", \"Accuracy\"]\n",
    "table.add_row([\"Logistic Regression\", round(accuracy_score(result[\"y\"], result[\"Logistic Regression\"]),2)])\n",
    "table.add_row([\"Random Forest\", round(accuracy_score(result[\"y\"], result[\"Random Forest\"]),2)])\n",
    "table.add_row([\"Boosted Trees\", round(accuracy_score(result[\"y\"], result[\"Gradient Boosted Trees\"]),2)])\n",
    "table.add_row([\"SVM\", round(accuracy_score(result[\"y\"], result[\"SVM\"]),2)])\n",
    "table.add_row([\"Neural Network\", round(accuracy_score(result[\"y\"], result[\"Neural Net\"]),2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+\n",
      "|        Method       | Accuracy |\n",
      "+---------------------+----------+\n",
      "| Logistic Regression |   0.73   |\n",
      "|    Random Forest    |   0.83   |\n",
      "|    Boosted Trees    |   0.8    |\n",
      "|         SVM         |   0.72   |\n",
      "|    Neural Network   |   0.75   |\n",
      "+---------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
